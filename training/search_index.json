[
["index.html", "NCEAS Data Team Training Welcome to NCEAS! First day to-dos Account information NCEAS Events", " NCEAS Data Team Training Jeanette Clark, Jesse Goldstein, Dominic Mullen, Bryce Mecum, Matt Jones, Peter Slaughter, Irene Steves, Mitchell Maier, Emily O’Dean 2018-10-11 Welcome to NCEAS! First day to-dos Get a tour of the office from Ginger, Dom, Jeanette, or Jesse Fill out required paperwork from Michelle Have Ana take your picture (room 309) Set up the remainder of your accounts Account information LDAP - NCEAS - Jeanette or Jesse should have set this up prior to your start date to help get other accounts set up by the first day. This account and password control your access to: arcticdata RT queue GitHub Enterprise - arctic-data and sasap-data Datateam server - follow instructions in email from Nick at NCEAS to reset your datateam password in the terminal ORCiD - create an account NCEAS Slack - get an invite from slack.nceas.ucsb.edu Channels to join: #arctica, #arcticbot, #computing, #datateam, #devteam, #sasap, #social Arctic Data Center Team - after creation of ORCiD and sign-in to both arcticdata.io and test.arcticdata.io, request addition to the admin group from Dom, Jeanette, or Jesse in Slack GitHub - if you do not have a public GitHub account already, please register for one here Schedule - fill out anticipated quarterly schedule. Note there are separate tabs for each week AND a general schedule tab for the quarter. NCEAS Events NCEAS hosts a number of events that you are encouraged to attend. Keep an eye on your email but the recurring events are: Roundtable weekly presentation and discussion of research by a visiting or local scientist Wednesdays at 12:15 in the lounge Coffee Klatch coffee, socializing, and news updates for NCEAS Tuesdays at 10:30 in the lounge Salad Potluck potluck salad and socializing, bring a topping or side to share! second Tuesday of the month, 12:15 in the lounge "],
["introduction-to-open-science.html", "Chapter 1 Introduction to open science 1.1 Open science background reading 1.2 Effective data management 1.3 Using DataONE 1.4 Working on a remote server 1.5 Cyberduck instructions 1.6 A note on paths 1.7 A note on scripts 1.8 Exercise 1", " Chapter 1 Introduction to open science These materials are meant to introduce you to the principles of open science, effective data management, and data archival with the DataONE data repository. 1.1 Open science background reading Read the content on the Arctic Data Center (ADC) webpage to learn more about data submission, preservation, and the history of the ADC. We encourage you to follow the links within these pages to gain a deeper understanding. about submission preservation history 1.2 Effective data management Read Matt Jones et al.’s paper on effective data management to learn how we will be organizing datasets prior to archival. (Please note that while the tips outlined in this article are best practices, we often do not reformat data files submitted to our repositories unless necessary. It is best to be conservative and not alter other people’s data without good reason.) 1.3 Using DataONE Data Observation Network for Earth (DataONE) is a community driven project providing access to data across multiple member repositories, supporting enhanced search and discovery of Earth and environmental data. Watch the first 38 minutes of this video explaining how DataONE works. This video is pretty technical, and you may not understand it all at first. Please feel free to ask Dom, Jeanette, or Jesse questions. 1.4 Working on a remote server All of the work that we do at NCEAS is done on our remote server, datateam.nceas.ucsb.edu. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. We access RStudio on our server through this link. To transfer files on and off of the server, you’ll need to use either bash commands in the terminal, or an FTP client. We use a client called Cyberduck. 1.5 Cyberduck instructions To use Cyberduck to transfer local files onto the Datateam server: Open Cyberduck. Check that you have the latest version (Cyberduck -&gt; Check for Update…). If not, download and install the latest (you may need Dom, Jesse, or Jeanette to enter a password). Click “Open Connection”. From the drop-down, choose “SFTP (Secure File Transfer Protocol)”. Enter “datateam.nceas.ucsb.edu” for Server. Enter your username and password. Connect. From here, you can drag and drop files to and from the server. 1.6 A note on paths On the servers, paths to files in your folder always start with /home/yourusername/.... When you write scripts, try to avoid writing relative paths (which rely on what you have set your working directory to) as much as possible. Instead, write out the entire path as shown above, so that if another data team member needs to run your script, it is not dependent on a working directory. 1.7 A note on scripts To make it easy to follow the flow of your work, it may help to number your scripts. For example, 01_clean_data.R, 02_edit_EML.R, 03_publish.R. Check out Jenny Bryan’s slidedeck on naming things for more on this. 1.8 Exercise 1 Download the csv of Table 1 from this paper. Reformat the table using R under the guidelines in the journal article on effective data management. If you need an R refresher, take as much time as you need to go over the data carpentry guide.. You may also find the data carpentry lesson on dplyr and OHI’s data wrangling chapters helpful. Go to test.arcticdata.io and submit your reformatted file with appropriate metadata that you derive from the text of the paper: list yourself as the first ‘Creator’ so your test submission can easily be found, for the purposes of this training exercise, not every single author needs to be listed with full contact details, listing the first two authors is fine, directly copying and pasting sections from the paper (abstract, methods, etc.) is also fine, attributes (column names) should be defined, including correct units and missing value codes. "],
["create-data-packages-in-r.html", "Chapter 2 Create data packages in R 2.1 What is in a package? 2.2 About identifiers 2.3 Uploading packages using R 2.4 Publish an object 2.5 Create a resource map 2.6 Exercise 2", " Chapter 2 Create data packages in R This chapter will teach you how to create and submit a data package to a DataONE node in R. 2.1 What is in a package? A data package generally consists of at least 3 “objects” or files. Metadata: One object is the metadata file itself. In case you are unfamiliar with metadata, metadata are information that describe data (e.g. who made the data, how were the data made, etc.). The metadata file will be in an XML format, and have the extension .xml (extensible markup language). We often refer to this file as the EML (Ecological Metadata Language), which is the metadata standard that it uses. Data: Other objects in a package are the data files themselves. Most commonly these are data tables (.csv), but they can also be audio files, NetCDF files, plain text files, PDF documents, image files, etc. Resource Map: The final object is the resource map. This object is a plain text file with the extension .rdf (Resource Description Framework) that defines the relationships between all of the other objects in the data package. It says things like “this metadata file describes this data file,” and is critical to making a data package render correctly on the website with the metadata file and all of the data files together in the correct place. Fortunately, we rarely, if ever, have to actually look at the contents of resource maps; they are generated for us using tools in R. 2.2 About identifiers Each object (metadata files, data files, resource maps) on the ADC or the KNB (another repo) has a unique identifier, also sometimes called a “PID” (persistent identifier). When you look at the landing page for a data set, for example here, you can find the resource map identifier listed under the title in the gray bar after the words “Files in this dataset Package:” (resource_map_doi:10.18739/A2836Z), the metadata identifier in the “General &gt; Identifier” section of the metadata record or after the title with blue font (doi:10.18739/A2836Z), and the data identifier by clicking the “more info” link next to the data object, and looking at the “Online Distribution Info” section (arctic-data.9546.1). Different versions of a package are linked together by what we call the “version chain” or “obsolescence chain”. Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the new version of the updated object. When making changes to a package, always use the arcticdatautils::update_object or arcticdatautils::publish_update commands on the latest versions of all objects to ensure that the version chain is maintained. 2.3 Uploading packages using R We will be using R to connect to the NSF Arctic Data Center (ADC) data repository to push and pull edits. To identify yourself as an admin you will need to pass a ‘token’ into R. Do this by signing in to the ADC with your ORCiD, hovering over your name and clicking on “My profile”, then navigating to “Settings” and “Authentication Token”, copying the “Token for DataONE R”, and pasting and running it in your R console. This token is your identity on these sites, please treat it as you would a password (ie. don’t paste into scripts that will be shared). The easiest way to do this is to always run the token in the console. There’s no need to keep it in your script since it’s temporary anyway. Sometimes you’ll see a placeholder in scripts to remind users to get their token, such as: options(dataone_test_token = &quot;...&quot;) Next, please be sure these packages are loaded: library(devtools) library(dataone) library(datapack) library(EML) library(remotes) library(XML) If any package could not be loaded, use the following command (replacing package_name with the actual package name) to install the package, then load them. install.packages(&quot;package_name&quot;) Now install a couple of packages: remotes::install_github(&quot;nceas/arcticdatautils&quot;) library(arcticdatautils) remotes::install_github(&quot;nceas/datamgmt&quot;) library(datamgmt) For this training, we will be working exclusively on the Arctic test site, or “node.” In many of the functions you will use this will be the first argument. It is often referred to in documentation as mn, short for member node. Set the node to the test Arctic node: cn_staging &lt;- CNode(&#39;STAGING&#39;) adc_test &lt;- getMNode(cn_staging,&#39;urn:node:mnTestARCTIC&#39;) 2.4 Publish an object Objects (data files, xml metadata files) can be published to a DataONE node using the function publish_object from the arcticdatautils R Package. To publish an object, you must first get the formatId of the object you want to publish. A few common formatIds are listed below. # .csv file formatId &lt;- &quot;text/csv&quot; # .txt file formatId &lt;- &quot;text/plain&quot; # metadata file formatId &lt;- &quot;eml://ecoinformatics.org/eml-2.1.1&quot; # OR formatId &lt;- format_eml() Most objects have registered formatIds that can be found on the DataONE website here. Metadata files (as shown above) use a special function to set the formatId. If the formatId is not listed at the DataONE website, you can set formatId &lt;- &quot;application/octet-stream&quot;. Once you know the formatId you can publish your object using these commands: path &lt;- &quot;path/to/your/file&quot; formatId &lt;- &quot;your/formatId&quot; pid &lt;- publish_object(adc_test, path = path, format_id = formatId) After publishing the object, the PID will need to be added to a resource map by updating or creating a resource map. Additionally, the rights and access for the object must be set. However, you only need to give other people rights and access to objects to objects that are not yours, for the training you don’t need to do this. 2.5 Create a resource map If you are creating a new data package, you must create a resource map. Resource maps provide information about the resources in the data package (e.g. what data files should be included in the package, where the metadata are, etc.). To create a new resource map with an existing published metadata_pid and data_pids, use the following command: resource_map_pid &lt;- create_resource_map(adc_test, metadata_pid = metadata_pid, data_pids = data_pids) 2.6 Exercise 2 Locate the data package you published in Exercise 1 by navigating to the “My Profile &gt; My Data” section on test.arcticdata.io. Download the metadata and data files and transfer them to the Datateam server. Using the functions described in the section above, publish your metadata record and data file to the site. (When you do so, make sure you save the PIDs to different variables in R…) i.e. data_pid &lt;- publish_object(...) metadata_pid &lt;- publish_object(...) Then create a resource map containing your data and metadata. View your new data set (which is identical to the one you created previously) by appending the metadata PID to the end of the URL test.arcticdata.io/#view/… "],
["explore-eml.html", "Chapter 3 Explore EML 3.1 Navigate through EML 3.2 Understand the EML schema 3.3 Access specific elements", " Chapter 3 Explore EML We use the Ecological Metadata Language (EML) to store structured metadata for all datasets submitted to the Arctic Data Center. EML is written in XML (extensible markup language) and functions for building and editing EML are in the EML R package. For additional background on EML and principles for metadata creation, check out this paper. 3.1 Navigate through EML The first task when editing an EML file is navigating the EML file. An EML file is organized in a structure that contains many lists nested within other lists. The function EML::eml_view (make sure to install and load the listviewer package if you don’t have it already) allows you to get a crude view of an EML file in the viewer. It can be useful for exploring the file. eml_raw &lt;- rawToChar(getObject(adc, &quot;doi:10.18739/A2FS1H&quot;)) eml_view(eml_raw) # you can also pass a path to an EML file To navigate this complex structure in R, use the @ symbol. The @ symbol allows you to go deeper into the EML structure and to see what slots are nested within other slots. However, you have to tell R where you want to go in the structure when you use the @ symbol. For example, if you want to go into the data set of your EML you would use the command eml@dataset. If you want to go to the creators of your data set you would use eml@dataset@creator. Note here that creators are contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing @ and a list of available locations in the structure will appear (e.g., eml@&lt;TAB&gt;): Note that if you hit tab, and the only option is .Data, this most likely implies that you are trying to go deeper within a list. For example eml@dataset@creator@&lt;TAB&gt; will return only .Data. This is because creator is a list object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is the index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now eml@dataset@creator[[1]]@&lt;TAB&gt; will give you many more options. Note, .Data sometimes means you have reached the end of a branch in the EML structure. At this point stop and take a deep breath. The key takeaway is that EML is a hierarchical tree structure. The best way to get familiar with it is to explore the structure. Try entering eml@dataset into your console, and print it. Now make the search more specific, for instance: eml@dataset@abstract. 3.2 Understand the EML schema Another great resource for navigating the EML structure is looking at the schema which defines the structure. The .png files on this page show the schema as a diagram. Additional information on the schema and how different elements (or “slots”) are defined can be found here). Further explanations of the symbology can be found here. The schema is complicated and may take some time to get familiar with before you will be able to fully understand it. For example, let’s take a look at eml-party. To start off, notice that some elements are in solid boxes, whereas others are in dashed boxes. A solid box indicates that the element is required if the element above it (to the left in the schema) is used, whereas a dashed box indicates that the element is optional. Notice also that below the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname. You will also see icons linking the EML slots together, which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema. The eml schema sections you may find particularly helpful include eml-party, eml-attribute, and eml-physical. (Click “Download” to zoom in.) For a more detailed description of the EML schema, see the reference section on exploring EML. 3.3 Access specific elements The eml_get function is a powerful tool for exploring EML (more on that here). It takes any chunk of EML and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples: eml &lt;- read_eml(system.file(&quot;example-eml.xml&quot;, package = &quot;arcticdatautils&quot;)) eml_get(eml, &quot;creator&quot;) [[1]] An object of class &quot;ListOfcreator&quot; [[1]] &lt;creator system=&quot;uuid&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;National Center for Ecological Analysis and Synthesis&lt;/organizationName&gt; &lt;/creator&gt; eml_get(eml, &quot;boundingCoordinates&quot;) &lt;boundingCoordinates&gt; &lt;westBoundingCoordinate&gt;-135&lt;/westBoundingCoordinate&gt; &lt;eastBoundingCoordinate&gt;-134&lt;/eastBoundingCoordinate&gt; &lt;northBoundingCoordinate&gt;59&lt;/northBoundingCoordinate&gt; &lt;southBoundingCoordinate&gt;57&lt;/southBoundingCoordinate&gt; &lt;/boundingCoordinates&gt; eml_get(eml, &quot;url&quot;) [1] &quot;ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456&quot; You can also use the which_in_eml function from the datamgmt package to get indices within an EML list. Here are some examples: # Question: Which creators have a surName &quot;Mecum&quot;? n &lt;- which_in_eml(eml@dataset@creator, &quot;surName&quot;, &quot;Mecum&quot;) # Answer: eml@dataset@creator[n] # Question: Which dataTables have an entityName that begins with &quot;2016&quot; n &lt;- which_in_eml(eml@dataset@dataTable, &quot;entityName&quot;, function(x) {grepl(&quot;^2016&quot;, x)}) # Answer: eml@dataset@dataTable[n] # Question: Which attributes in dataTable[[1]] have a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable[[1]]@attributeList@attribute, &quot;numberType&quot;, &quot;natural&quot;) # Answer: eml@dataset@dataTable[[1]]@attributeList@attribute[n] #&#39; # Question: Which dataTables have at least one attribute with a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable, &quot;numberType&quot;, function(x) {&quot;natural&quot; %in% x}) # Answer: eml@dataset@dataTable[n] "],
["edit-eml-in-r.html", "Chapter 4 Edit EML in R 4.1 Get package and EML 4.2 Edit an EML element 4.3 Edit attributeLists 4.4 Set physical 4.5 Edit dataTables 4.6 Edit otherEntities 4.7 Edit spatialVectors 4.8 Set coverages 4.9 Set methods 4.10 Set parties 4.11 Validate EML and update package 4.12 SASAP package workflows 4.13 Exercise 3", " Chapter 4 Edit EML in R This chapter is a practical tutorial for using R to read, edit, write, and validate EML documents. Much of the information here can also be found in the vignettes for the R packages used in this section (e.g. the EML package). 4.1 Get package and EML Before we look more in depth at EML, we first need to load your data package into R. After setting your node, use the following commands to load the package: rm_pid &lt;- &quot;your_resource_map_pid&quot; pkg &lt;- get_package(adc_test, rm_pid, file_names = TRUE) After loading the package, you can also load the EML file into R using the following command: eml &lt;- read_eml(getObject(adc_test, pkg$metadata)) Tip to always have the most recent resource map. When editing data packages, you always want to be working with the most recent update. To ensure you have the most recent resource map, you can use the following commands: rm_pid_original &lt;- &quot;your_original_resource_map_pid&quot; all_rm_versions &lt;- get_all_versions(adc_test, rm_pid_original) rm_pid &lt;- all_rm_versions[length(all_rm_versions)] print(rm_pid) 4.2 Edit an EML element There are multiple ways to edit an EML element. 4.2.1 Edit EML with strings The most basic way to edit an EML element would be to go into the EML schema to the location of a character string and then replace that character string with a new one. For example, to change the title one could use the following commands: new_title &lt;- &quot;New Title&quot; eml@dataset@title[[1]]@.Data &lt;- new_title However, this isn’t the best method to edit the EML (unless you are an expert in both S4 objects and the EML schema) since the nesting and lists of elements can get very complex. 4.2.2 Edit EML with the “EML” package For editing simple text sections, a better option is to use EML::read_eml. To use this function, one would first run the slot that is in need of editing. For example, for a title this would involve calling eml@dataset@title[[1]] and then copying the result. In this case, the result will be of the form &lt;title&gt;Title Text Here&lt;/title&gt;. To make a new title, one would replace the text between the &lt;title&gt;&lt;/title&gt; tags using a similar workflow as below. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title[[1]] &lt;- new_title Note, without specifying which title with [[1]], the following code will give you the error Error in (function (cl, name, valueClass) : assignment of an object of class “title” is not valid for @‘title’ in an object of class “dataset”; is(value, &quot;ListOftitle&quot;) is not TRUE. # Bad Example new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- new_title The above gives an error because eml@dataset@title is a slot for a list and new_title is a single object. Therefore you must either specify which title you want to replace as was done above by specifying the first title in the list with [[1]] or turn new_title into a list/vector utilizing the c() command as follows. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- c(new_title) However, if there were multiple titles, the above would replace all the titles with the single title. This behavior may or may not be desirable so be careful. One final note, a benefit of this method to edit EML objects is that advanced text features can be easily added using this workflow. 4.2.3 Edit EML with objects A final way to edit an EML element would be to build a new object to replace the old object. To begin, you must determine the class of the object you want to edit (generally this is just the schema name of the object). The function class() is helpful here. For example, if you want to edit eml@dataset@title[[1]] use the following command to find the class: class(eml@dataset@title[[1]]) The result shows that this object has a class title. Therefore you must replace it with an object of class title. To do so, use as(). To use as(), input the desired string followed by the desired class. new_title &lt;- as(&quot;New Title 3&quot;, &quot;title&quot;) eml@dataset@title[[1]] &lt;- new_title #or eml@dataset@title &lt;- c(new_title) Note that if you want to create an object with nested objects, you may have to use the command new() which is similar to as() but with the order of specifying values and class switched. See help on editing datatables for an example of when to use new(). 4.3 Edit attributeLists Attributes are stored in an attributeList. When editing attributes in R, you need to create one to three objects: A data.frame of attributes A data.frame of custom units (if applicable) A data.frame of factors (if applicable) Attributes can exist in EML for dataTable, otherEntity, and spatialVector data objects. Please note that submitting attribute information through the website will store them in an otherEntity object by default. We prefer to store them in a dataTable object for tabular data or a spatialVector object for spatial data. To edit or examine an existing attribute table already in an EML file, you can use the following commands: # If they are stored in an otherEntity (submitted from the website by default) attributeList &lt;- EML::get_attributes(eml@dataset@otherEntity[[i]]@attributeList) # Or if they are stored in a dataTable (usually created by a datateam member) attributeList &lt;- EML::get_attributes(eml@dataset@dataTable[[i]]@attributeList) # Or if they are stored in a spatialVector (usually created by a datateam member) attributeList &lt;- EML::get_attributes(eml@dataset@spatialVector[[i]]@attributeList) attributes &lt;- attributeList$attributes print(attributes) 4.3.1 Edit Attributes Attribute information should be stored in a data.frame with the following columns: attributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp” attributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)” attributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.” measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. e.g.: Low, Medium, High dateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001 ratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second. interval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or latitude and longitude coordinates, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius. domain: One of: textDomain, enumeratedDomain, numericDomain, dateTime. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTime, NA otherwise. Format string for dates, e.g. “DD/MM/YYYY”. definition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases. unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found here. numberType: Required for numericDomain, NA otherwise. Options are real, natural, whole, and integer. real: positive and negative fractions and integers (…-1,-0.25,0,0.25,1…) natural: non-zero positive integers (1,2,3…) whole: positive integers and zero (0,1,2,3…) integer: positive and negative integers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. You can create attributes manually by typing them out in R following a workflow similar to the one below: attributes &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Sample number&#39;, NA, NA, NA, &#39;comments about sampling process&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;), stringsAsFactors = FALSE) However, typing this out in R can be a major pain. Luckily, there is a Shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you), to edit an existing attribute table, or to create attributes from scratch. Use the following commands to create or modify attributes (these commands will launch a Shiny app in your web browser): # From data (recommended) datamgmt::create_attributes_table(data = data) # From an existing attribute table datamgmt::create_attributes_table(attributes_table = attributes_table) # From scratch datamgmt::create_attributes_table() Once you are done editing a table in the app, click the Print button to print text of a code that will build a data.frame in R. Copy that code and assign it to a variable in your script (e.g. attributes &lt;- data.frame(...)). For simple attribute corrections, datamgmt::edit_attribute allows you to edit the slots of a single attribute within an attribute list. 4.3.2 Edit Custom Units EML has a set list of units that can be added to an EML file. These can be seen by using the following code: standardUnits &lt;- EML::get_unitList() View(standardUnits$units) If you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields: id: The unit id (ids are camelCased) unitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes) parentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”) multiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000) name: Unit abbreviation (e.g. for kilometer name = “km”) description: Text defining the unit (e.g. for kilometer description = “1000 meters”) Additionally, datamgmt::create_attributes_table will tell you if each of your units are standard or not. If your unit is not standard, you should use the following code to help auto-generate a custom unit: datamgmt::return_eml_units(&quot;your_unit&quot;) Note that datamgmt::create_attributes_table calls datamgmt::return_eml_units for you! datamgmt::return_eml_units will auto-generate many of these fields for you (but don’t just assume the auto-generation will be perfect; always ensure the auto-generation correctly handles your unit.) 4.3.3 Edit Factors For attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition. attributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute. code should contain all unique values of the given attributeName that exist within the actual data. definition should contain a plain text definition that describes each code. There is a tab in the datamgmt::create_attributes_table app that will help you build factors. If you need to build factors by hand, you can use named character vectors and then convert them to a data.frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region”. Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a data.frame using the names of the named character vectors and their definitions. factors &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) 4.3.4 Finalize attributeList Once you have built your attributes, factors, and custom units, you can add them to EML objects. Attributes and factors are combined to form an attributeList using the following command: attributeList &lt;- EML::set_attributes(attributes = attributes, factors = factors) This attributeList must then be added to a dataTable. Custom units are added to additionalMetadata using the following command: unitlist &lt;- set_unitList(custom_units) eml@additionalMetadata &lt;- c(as(unitlist, &quot;additionalMetadata&quot;)) 4.4 Set physical To set the physical aspects of a data object, use the following commands to build a physical object from a data PID that exists in your package. Remember to set the member node to test.arcticdata.io ! physical &lt;- arcticdatautils::pid_to_eml_physical(mn, pkg$data[[i]]) Alternatively, you can set the physical of a data object not yet in your package by simply inputting the data PID: physical &lt;- arcticdatautils::pid_to_eml_physical(mn, &quot;your_data_pid&quot;) The physical must then be assigned to the data object. A final, but not recommended option, is to set the physical by hand. To do so, one can use a workflow similar to the one below. However, the far superior workflow is to publish or update your data first and then use pid_to_eml_physical to set the physical. id &lt;- &#39;your_data_pid&#39; # this should be an actual PID path &lt;- &#39;~/your/data/path&#39; # path to data table physical &lt;- EML::set_physical(objectName = &#39;your_file_name&#39;, id = id, size = as.character(file.size(path)), sizeUnit = &#39;bytes&#39;, authentication = digest(path, algo=&quot;sha1&quot;, serialize=FALSE, file=TRUE), authMethod = &#39;SHA-1&#39;, numHeaderLines = &#39;1&#39;, fieldDelimiter = &#39;,&#39;, url = paste0(&#39;https://cn.dataone.org/cn/v2/resolve/&#39;, id)) 4.5 Edit dataTables To edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable with the new() command as follows: dataTable &lt;- new(&quot;dataTable&quot;, entityName = &quot;A descriptive name for the data (does not need to be the same as the data file)&quot;, entityDescription = &quot;A description of the data&quot;, physical = physical, attributeList = attributeList) The dataTable must then be set to the EML (i.e.: eml@dataset@dataTable[[i]] &lt;- dataTable). 4.6 Edit otherEntities 4.6.1 Remove otherEntities To remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable. eml@dataset@otherEntity[[i]] &lt;- NULL 4.6.2 Create otherEntities If you need to create/update an otherEntity, make sure to publish or update your data object first (if it is not already on the DataONE MN). Then build your otherEntity. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, pkg$data[[i]]) Alternatively, you can build the otherEntity of a data object not in your package by simply inputting the data PID. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, &quot;your_data_pid&quot;, entityType = &quot;otherEntity&quot;, entityName = &quot;Entity Name&quot;, entityDescription = &quot;Description about entity&quot;) The otherEntity must then be set to the EML, like so: eml@dataset@otherEntity &lt;- otherEntity If you have additional otherEntity objects in the EML already, you will need to add the new one like this: eml@dataset@otherEntity[[i]] &lt;- otherEntity Where i is set to the number of existing entities plus one. 4.7 Edit spatialVectors Occasionally, you may encounter a third type of data object: spatialVector. This object contains spatial data, such as a shapefile or geodatabase. Editing a spatialVector is similar to editing a dataTable or an otherEntity. A physical and attributeList should be present. One important difference is that a spatialVector object should also have a geometry slot that describes the geometry features of the data. The possible values include one or more (in a list) of ‘Point’, ‘LineString’, ‘LinearRing’, ‘Polygon’, ‘MultiPoint’, ‘MultiLineString’, ‘MultiPolygon’, or ‘MultiGeometry’. To add a geometry slot use: eml@dataset@spatialVector[[1]]@geometry[[1]] &lt;- read_eml(&quot;&lt;geometry&gt;Polygon&lt;/geometry&gt;&quot;) Additionally, spatial data should typically be archived within a .zip file to ensure all related and interdependent files stay together. For example, a spatial dataset for a shapefile should, at a minimum, consist of separate .dbf, .shp, and .shx files with the same prefix in the same directory. All these files are required in order to use the data. Also note that shapefiles limit attribute names to 10 characters, so attribute names in the metadata may not match exactly to attribute names in the data. Here is an example of what spatialVector metadata should look like, including physical, attributeList, and geometry slots: &lt;spatialVector system=&quot;uuid&quot;&gt; &lt;entityName&gt;sasap_regions.zip&lt;/entityName&gt; &lt;entityDescription&gt;Contains the shapefile depicting the SASAP regions. Zip contains .cpg, .dbf, .prj, .shp, and .shx files.&lt;/entityDescription&gt; &lt;physical scope=&quot;document&quot;&gt; &lt;objectName&gt;sasap_regions.zip&lt;/objectName&gt; &lt;size unit=&quot;bytes&quot;&gt;2533992&lt;/size&gt; &lt;authentication method=&quot;SHA1&quot;&gt;2d199f6f1f5f5b36525d1cf1019c0a4551b98762&lt;/authentication&gt; &lt;dataFormat&gt; &lt;externallyDefinedFormat&gt; &lt;formatName&gt;application/zip&lt;/formatName&gt; &lt;/externallyDefinedFormat&gt; &lt;/dataFormat&gt; &lt;distribution scope=&quot;document&quot;&gt; &lt;online&gt; &lt;url function=&quot;download&quot;&gt;https://cn.dataone.org/cn/v2/resolve/urn:uuid:f6ab206b-312c-4caf-89c8-89eb9d031aac&lt;/url&gt; &lt;/online&gt; &lt;/distribution&gt; &lt;/physical&gt; &lt;attributeList&gt; &lt;attribute&gt; &lt;attributeName&gt;region_id&lt;/attributeName&gt; &lt;attributeDefinition&gt;SASAP region ID&lt;/attributeDefinition&gt; &lt;measurementScale&gt; &lt;interval&gt; &lt;unit&gt; &lt;standardUnit&gt;dimensionless&lt;/standardUnit&gt; &lt;/unit&gt; &lt;numericDomain&gt; &lt;numberType&gt;natural&lt;/numberType&gt; &lt;/numericDomain&gt; &lt;/interval&gt; &lt;/measurementScale&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;attributeName&gt;region&lt;/attributeName&gt; &lt;attributeDefinition&gt;SASAP region name&lt;/attributeDefinition&gt; &lt;measurementScale&gt; &lt;nominal&gt; &lt;nonNumericDomain&gt; &lt;textDomain&gt; &lt;definition&gt;SASAP region name&lt;/definition&gt; &lt;/textDomain&gt; &lt;/nonNumericDomain&gt; &lt;/nominal&gt; &lt;/measurementScale&gt; &lt;/attribute&gt; &lt;/attributeList&gt; &lt;geometry&gt;Polygon&lt;/geometry&gt; &lt;/spatialVector&gt; 4.8 Set coverages Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a data set. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering eml@dataset@coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage. coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;), geographicDescription = &quot;The geographic region covers the lake region near Eagle Mountain.&quot;, west = -154.6192, east = -154.5753, north = 68.3831, south = 68.3619) eml@dataset@coverage &lt;- coverage You can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages: geocov1 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 1&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 68), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -154), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 67), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -155))) geocov2 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 2&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 65), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -155), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 64), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -156))) coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;)) eml@dataset@coverage@geographicCoverage &lt;- c(geocov1, geocov2) To set taxonomic coverage: # add each new element as a tax object tax1 &lt;- new(&quot;taxonomicClassification&quot;, taxonRankName = new(&quot;taxonRankName&quot;, &quot;Species&quot;), taxonRankValue = new(&quot;taxonRankValue&quot;, &quot;Calamagrostis deschampsioides&quot;)) tax2 &lt;- new(&quot;taxonomicClassification&quot;, taxonRankName = new(&quot;taxonRankName&quot;, &quot;Species&quot;), taxonRankValue = new(&quot;taxonRankValue&quot;, &quot;Carex aquatilis&quot;)) # combine all tax elements into taxonomic coverage object taxcov &lt;- new(&quot;taxonomicCoverage&quot;, taxonomicClassification = c(tax1, tax2)) eml@dataset@coverage@taxonomicCoverage &lt;- c(taxcov) 4.9 Set methods The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the new command. Remember you can explore possible slots within an element by creating an empty object of the class you are trying to create. For example, method_step &lt;- new('methodStep'), and using auto-complete on method_step@. Potentially the most useful way to set methods is by editing with the EML package. Another simple and potentially useful way to add methods to an EML that has no methods at all is by adding them via a MS Word document. An example is shown below: methods1 &lt;- set_methods(&#39;methods_doc.docx&#39;) eml@dataset@methods &lt;- methods1 If you want to make minor changes to existing methods information that has a lot of nested elements, your best bet may be to edit the EML manually in a text editor (or in RStudio), otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing methods information. 4.9.1 Adding sampling info to methods section # add method steps as new variables step1 &lt;- new(&#39;methodStep&#39;, description = &quot;methods text&quot;) stEx &lt;- new(&quot;studyExtent&quot;, description = &quot;study extent description&quot;) samp &lt;- new(&quot;sampling&quot;, studyExtent = stEx, samplingDescription = &quot;sampling description text&quot;) # combine all methods steps and sampling info methods1 &lt;- new(&quot;methods&quot;, methodStep = c(step1), sampling = samp) eml@dataset@methods &lt;- methods1 4.10 Set parties To add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associatedParty classes. NCEASadd &lt;- new(&quot;address&quot;, deliveryPoint = &quot;735 State St #300&quot;, city = &quot;Santa Barbara&quot;, administrativeArea = &#39;CA&#39;, postalCode = &#39;93101&#39;) The creator, contact, and associatedParty classes can easily be created using functions from the arcticdatautils package. Here, we use eml_creator to set our data set creator. JC_creator &lt;- arcticdatautils::eml_creator(given_names = &quot;Jeanette&quot;, sur_name = &quot;Clark&quot;, organization = &quot;NCEAS&quot;, email = &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@creator &lt;- c(JC_creator) Similarly, we can set the contacts. In this case, there are two, so we set eml@dataset@contact as a ListOfcontact, which contains both of them. JC_contact &lt;- arcticdatautils::eml_contact(given_names = &quot;Jeanette&quot;, sur_name = &quot;Clark&quot;, organization = &quot;NCEAS&quot;, email = &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) JG_contact &lt;- arcticdatautils::eml_contact(given_names = &quot;Jesse&quot;, sur_name = &quot;Goldstein&quot;, organization = &quot;NCEAS&quot;, email = &quot;jgoldstein@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@contact &lt;- c(JC_contact, JG_contact) Finally, the associatedPartys are set. Note that associatedPartys MUST have a role defined, unlike creator or contact. JG_ap &lt;- arcticdatautils::eml_associated_party(given_names = &quot;Jesse&quot;, sur_name = &quot;Goldstein&quot;, organization = &quot;NCEAS&quot;, email = &quot;jgoldstein@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, address = NCEASadd, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, role = &quot;metaataProvider&quot;) eml@dataset@associatedParty &lt;- c(JG_ap) 4.11 Validate EML and update package To make sure that your edited EML is valid against the EML schema, run eml_validate on your EML. Fix any errors that you see, and then save your EML to a path of your choice or a temp file. You will later pass this path as an argument to update the package. eml_validate(eml) eml_path &lt;- &quot;path/to/save/eml.xml&quot; write_eml(eml, eml_path) To update a package with the newly edited EML, use arcticdatautils::publish_update. This function has an argument for adding data PIDs (or otherwise including existing data PIDs) to make sure that they stay with the package. This function allows you to make metadata edits, as well as add or remove data objects (discussed in the following chapter). update &lt;- publish_update(adc_test, metadata_pid = pkg$metadata, resource_map_pid = pkg$resource_map, data_pids = pkg$data, metadata_path = eml_path, public = FALSE) Note that there are other arguments to publish_update you may need. 4.12 SASAP package workflows Sometimes many data sets are associated with a larger project, such as the State of Alaska Salmon and People (SASAP) project. These data sets should be given additional project-specific information using eml@dataset@project. This will add pre-defined information including the project title, funding sources, and key personnel. You will also want to set access permissions to the project as well. If you are working on a SASAP data set, prior to writing the EML and publishing the data set you will set the project with this code: source(&#39;/home/eodean/sasap-data/data-submission/Helpers/SasapProjectCreator.R&#39;) eml@dataset@project &lt;- sasap_project() Next you should add SASAP-specific taxonomic coverage to the EML using the add_sasap_taxa function. This will ensure that any salmon species present in the dataset have their Latin and common names listed in the coverage and can be easily searched for. source(&#39;/home/dmullen/sasap-helpers/add_sasap_taxa.R&#39;) eml &lt;- add_sasap_taxa(mn, eml) Then, update the access permissions in the system metadata using set_rights_and_access. pkg &lt;- get_package(mn, resource_map_pid) set_rights_and_access(mn, unlist(pkg), &#39;CN=SASAP,DC=dataone,DC=org&#39;, permissions = c(&#39;read&#39;, &#39;write&#39;, &#39;changePermission&#39;)) Finally, go through the SASAP checklist to ensure that the package meets all of the project-specific requirements for publishing. After your package is published, run the qa_package() function from the datamgmt package. The function arguments are as follows: qa_package &lt;- function(node, pid, readAllData = TRUE, check_attributes = TRUE, check_creators = FALSE, check_access = FALSE) By default, qa_package checks for: Correctness of distribution URLs for each data object (URLs must match the EML physical section for the object) Congruence of metadata and data The check_creators and check_access flags can be set to TRUE to check: Correctness of ORCIDs of creators in a given EML Rights and access are set for creators in the system metadata In most cases, qa_package will be run just by passing in a member node and the resource map PID. qa_package(mn, resource_map_pid) 4.13 Exercise 3 Make sure your package from before is loaded into R. Replace the existing dataTable with a new dataTable object with an attributelist and physical section you write in R using the above commands. Then write, validate, and update your package. Use the checklist to review your submission. Make edits where necessary, and publish updates as needed. "],
["update-a-data-package.html", "Chapter 5 Update a data package 5.1 Update a data file 5.2 Update a package with a new data object 5.3 Exercise 4", " Chapter 5 Update a data package This chapter will teach you how to edit an existing data package in R. Earlier, we updated metadata. In this section we will learn how to update a data file, and how to update a package by adding an additional data file. 5.1 Update a data file To update a data file, you need to do three things: update the object itself, update the metadata that describes that object update the resource map (which affiliates the object with the metadata). The arcticdatautils::update_object function takes care of the first of these tasks. As arguments, update object takes the member node instance you are working in, the pid of the object to be updated, the path to the new version of that object on your computer, and the format id of the object. The code will look like this (very similar to the publish_object function call): id_new &lt;- update_object(adc_test, pid = &quot;the data pid you need to update&quot;, path = &quot;path/to/new/file.csv&quot;, format_id = &quot;text/csv&quot;) You will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Use line 2 (Id:) exactly, character for character. To acomplish the second task, you will need to update the metadata using the EML package. This is covered in Chapter 4. After you update a file, you will always need to update the metadata because parts of the physical section (such as the file size, checksum) will be different, and it may also require different attribute information. Once you have updated your metadata and saved it, you can update the package itself. 5.2 Update a package with a new data object Once you have updated the data objects and saved the metadata to a file, we can update the metadata and add the new pid to the resource map using publish_update. Make sure you have the package you want to update, loaded into R using get_package. 5.2.1 Publish update Now we can update your data package to include the new data object. eml_path &lt;- &quot;path/to/your/saved/eml.xml&quot; pkg &lt;- get_package(adc_test, &quot;resource_map_pid&quot;) update &lt;- publish_update(adc_test, metadata_pid = pkg$metadata, resource_map_pid = pkg$resource_map, data_pids = c(pkg$data, id_new), metadata_path = eml_path, public = FALSE) If a package is ready to be public, you can change the public argument in the publish_update call to TRUE. If you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you can change the use_doi argument to TRUE. This should only be done after the package is finalized and has been thoroughly reviewed! If the package has children, set the child_pids argument to pkg$child_packages. Refresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest. 5.3 Exercise 4 Locate the data package you published in the previous exercise by navigating to the URL test.arcticdata.io/#view/… Load the package and EML into R using the above commands. Load the data file associated with the package into R as a data.frame. (Hint: use read.csv() to upload the data file from your computer/the server.) Make an edit to the data in R (e.g. change one of the colnames to &quot;TEST&quot;). Save the edited data. (Hint: use write.csv(data, row.names = FALSE).) Update the data file in the package with the edited data file using the above commands. Update your package using the above commands. "],
["system-metadata.html", "Chapter 6 System metadata 6.1 Edit sysmeta 6.2 Set rights and access", " Chapter 6 System metadata Every object on the ADC (or on the KNB (the ‘Knowledge Network for Biocomplexity’)) has “system metadata”. An object’s system metadata have information about the file itself, such as the name of the file (fileName), the format (formatId), who the rightsHolder is, what the accessPolicy is, and more. Sometimes we will need to edit system metadata in order to make sure that things on the webpage display correctly, or to ensure a file downloads from the website with the correct file name and extension. Although the majority of system metadata changes that need to be made are done automatically, sometimes we need to change aspects of the system metadata (or ‘sysmeta’ for short) manually. 6.1 Edit sysmeta To edit the sysmeta of an object (data file, EML, or resource map, etc.) with a PID, first load the sysmeta into R using the following command: sysmeta &lt;- getSystemMetadata(mn, pid) Then edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command: sysmeta@fileName &lt;- &#39;NewFileName.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access, which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata to change either the formatId or fileName slots (see the DataONE list of format ids) for acceptable formats. # Example of setting the formatId slot sysmeta@formatId &lt;- &quot;eml://ecoinformatics.org/eml-2.1.1&quot; After you have changed the necessary slot, you can update the system metadata using the following command: updateSystemMetadata(mn, pid, sysmeta) 6.1.1 Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata are accurate when an object is first published). 6.1.2 Additional resources For a more in-depth (and technical) guide to sysmeta, check out the DataONE documentation: System Metadata Data Types in CICore 6.2 Set rights and access One final step when creating/updating packages is to make sure that the rights and access on all the objects that were uploaded are set correctly within the sysmeta. The function arcticdatautils::set_rights_and_access will set both, and arcticdatautils::set_access will just set access. There are two functions for this because a rightsHolder should always have access, but not all people who need access are rightsHolders. The rightsHolder of the data package is typically the submitter (if the data set is submitted through the web form (“editor”)), but if a data team member is publishing objects for a PI, the rightsHolder should be the main point of contact for the data set (i.e. the person who requested that we upload the data for them). To set the rights and access for all of the objects in a package, first get the ORCiD of the person to whom you are giving rights and access. You can set this manually, or grab it from one of the creators in an EML file. You can look up ORCID iDs here. # Manually set ORCiD subject &lt;- &#39;http://orcid.org/PUT0-YOUR-ORCD-HERE&#39; # Set ORCiD from EML creator subject &lt;- eml@dataset@creator[[1]]@userId[[1]]@.Data # As a convention we use `http:` instead of `https:` in our system metadata subject &lt;- sub(&quot;^https://&quot;, &quot;http://&quot;, subject) Note, when setting metadata, the ORCiD must start with http://. ORCiDs in EML should start with https://. The sub command above will change this formatting for you. Next, set the rights and access using the following command: set_rights_and_access(adc_test, pids = c(pkg$metadata, pkg$data, pkg$resource_map), subject = subject, permissions = c(&#39;read&#39;,&#39;write&#39;,&#39;changePermission&#39;)) If you ever need to remove/add public access to your package or object, you can use remove_public_read or set_public_read, respectively. remove_public_read(adc_test, c(pkg$metadata, pkg$data, pkg$resource_map)) 6.2.1 Exercise 5 Read the system metadata in from the data file you uploaded previously. Check to make sure the fileName and formatId are set correctly. Update the system metadata if necessary. Set the rights and access for all objects with your ORCiD. "],
["using-git-in-rstudio.html", "Chapter 7 Using Git in RStudio 7.1 Introduction 7.2 Setting up Git 7.3 Working with the repository 7.4 My Git tab disappeared", " Chapter 7 Using Git in RStudio 7.1 Introduction Important! If you have never used Git before, or only used it a little, or have no idea what is is, check out this intro to Git put together by the ecodatascience group at UCSB. Don’t worry too much about the forking and branching sections, as we will primarily be using the basic commit-pull-push commands. After you have read through that presentation, come back to this chapter. 7.1.1 So why do I need to use this again? There are several reasons why using the arctic-data GitHub repository is helpful, both for you and for the rest of the data team. Here are a few: Versioning: Did you accidentally make a change to your code and can’t figure out why it broke? Do you wish you could go back to the version that worked? If you add your code to the GitHub repo you can do this! Reproducibility: Being able to reproduce how you accomplished something is incredibly important. We should be able to tell anyone exactly how data have been reformatted, how metadata have been altered, and how packages have been created. As a data center, this is especially important for us with data team members that stay for 6-12 months because we may need to go back and figure out how something was done after the intern or fellow who wrote the code left the team. Troubleshooting: If you are building a particularly complicated EML, or doing some other advanced task, it is much easier for Jesse, Dom, Jeanette, or Bryce to troubleshoot your code if it is on the GitHub repo. We can view, troubleshoot, and fix bugs very easily when code is on the GitHub repo, with the added bonus of being able to go back a version if something should break. Solve future problems: Some of the issues we see in ADC submissions come up over and over again. When all of our code is on GitHub, we can easily reference code built for other submissions, instead of trying to solve the same problems over and over again from scratch. 7.2 Setting up Git Now you need to set up your Git global options, and tell it who you are. At the top of your RStudio window, select Tools &gt; Shell. In the prompt, you will need to run two commands, one at a time. The first tells Git what your name is, the second what your email address is. These are the commands: git config --global user.name &quot;My Name&quot; git config --global user.email myemail@domain.com After running these commands, the shell prompt should look like this: 7.2.1 Cloning the arctic-data repo Next, you need to clone the arctic-data repository to your RStudio. You do this by adding it as a “project”. In your RStudio window, click File &gt; New Project. Then click ‘Version Control’, and then select the ‘Git’ option. If you are prompted to save your workspace during this process, make sure all of your work is saved, and you don’t need anything in your environment, and then click ‘Don’t Save’. You should see a prompt asking you for a URL. Fill it out as shown in the image below to clone the arctic-data repository into the top level of your home directory. Note that the URL is the same URL you use to view the repository on the web. If you are using the sasap-data repository, the URL is http://github.nceas.ucsb.edu/NCEAS/sasap-data/. You will be prompted for your username and password, and then Git will clone the directory. The username/password you use should be the same one you use to log in when you go to http://github.nceas.ucsb.edu/KNB/arctic-data. Now you should have a directory called arctic-data in your RStudio files window. 7.3 Working with the repository 7.3.1 Adding a new script If you have been working on a script that you want to put in the arctic-data GitHub repo, you first need to save it somewhere in the arctic-data folder you cloned to your account on the Datateam server. You can do this by either moving your script into the folder or using the save-as functionality. Note that Git will try and version anything that you save in this folder, so you should be careful about what you save here. For our purposes, things that probably shouldn’t be saved in this folder include: Tokens: Any token file or script with a token in it should NOT be saved in the repository. Others could steal your login credentials if you put a token in GitHub. Data files: Git does not version data files very well. You shouldn’t save any .csv files or any other data files (including metadata). Workspaces/.RData: If you are in the habit of saving your R workspace, you shouldn’t save it in this directory. Plots/Graphics: For the same reasons as data files. Note: Do not EVER make a commit that you don’t understand. If something unexpected (like a file you have never worked on) shows up in your Git tab, ask Dom, Jesse, or Jeanette before committing. After you save your script in the appropriate place within the arctic-data folder, it will show up in your Git tab looking like this: Before you commit your changes, you need to click the little box under “Staged”. Do not stage or commit any .Rproj file. After clicking the box for your file, click “Commit” to commit your changes. In the window that pops up (you may need to force the browser to allow pop-ups), write your commit message. Always include a commit message. Remember that the commit message should be a concise description of the changes that were made to a file. Your window should look like this: Push ‘Commit’, and your commit will be saved to your local repository (this will not push it to the remote repository, yet). Now you want to merge the commits you made with the master version of the remote repository. You do this by using the command “Push.” But before you push, you always need to pull first to avoid merge conflicts. Pulling will merge the current version of the remote repository with your local repository, on your local machine. Click “Pull” and type in your credentials. Then, assuming you don’t have a merge conflict, you can push your changes by clicking “Push”. Always remember, the order is commit-pull-push. 7.3.2 Editing a script If you want to change a script, the workflow is the same. Just open the script that was saved in the arctic-data folder on your server account, make your changes, save the changes, stage them by clicking the box, commit, pull, then push to merge your version with the main version on the website. Do NOT edit scripts using the GitHub website. It is much easier to accidentally overwrite the history of a file this way. One thing you might be wondering as you are working on a script is, how often should I be committing my changes? It might not make sense to commit-pull-push after every single tiny change - if only because it would slow you way down. Personally, I commit every time I feel that a significant change has happened and that the chunk of code I was working on is “done”. Sometimes this is an entire script, other times it is just a few lines within a script. A good sign that you are committing too infrequently might be if many of your commit messages address a wide variety of coding tasks, such as: “wrote for loop to create referenced attribute lists for tables 1:20. also created nesting structure for this package with another package. also created attribute list for data table 40”. One final note, you can make multiple commits before you push to the repo. If you are making lots of changes to the script, you might want to make several commits before pull-push. You can see how many commits you are ahead of the “origin/master” branch (i.e. what you see on the website) by looking for text in your Git tab in RStudio that looks like this: 7.3.3 Where do I commit? The default right now is to save data-processing scripts in the arctic-data/datateam/data-processing/ directory, with sub-folders listed by project. Directories can be created as needed but please ask Dom, Jesse, or Jeanette first so we can try and maintain some semblance of order in the file structure. 7.4 My Git tab disappeared Sometimes R will crash so hard it loses your project information, causing your Git tab to disappear. Most likely, RStudio has just closed your “project” and all you need to do is reopen it. If your git tab has dissapeared, in the top right of your RStudio session, you should see a little R logo with “Project: (None)” next to it. This means you do not currently have a project open. Clicking the arrow should give you a dropdown menu of recent projects, where you can select “arctic-data” or “sasap-data.” Once you have opened your project, the git tab should reappear! This is also a convenient way to switch between projects if you are working in multiple repositories. "],
["using-rt.html", "Chapter 8 Using RT 8.1 Navigate RT", " Chapter 8 Using RT 8.1 Navigate RT The RT ticketing system is how we communicate with folks interacting with the Arctic Data Center. We use it for managing submissions, accessing issues, etc. It consists of three separate interfaces: Front Page All Tickets Ticket Page 8.1.1 Front page This is what you see first Home - brings you to this homepage Tickets - to search for tickets (also see number 5) Tools - not needed New Ticket - create a new ticket Search - Type in the ticket number to quickly navigate to a ticket Queue - Lists all of the tickets currently in a particular queue (such as ‘arcticdata’) and their statuses New = unopened tickets that require attention Open = tickets currently open and under investigation and/or being processed by a support team member Stalled = tickets awaiting responses from the PI/ submitter Tickets I Own - These are the current open tickets that are claimed by me Unowned Tickets - Newest tickets awaiting claim Ticket Status - Status and how long ago it was created Take - claim the ticket as yours 8.1.2 All tickets This is the queue interface from number 6 of the Front page 1. Ticket number and title 2. Ticket status 3. Owner - who has claimed the ticket 8.1.3 Example ticket Title - Include the PI’s name for reference Display - homepage of the ticket History - Comment/Email history, see bottom of Display page Basics - edit the title, status, and ownership here People - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters Links - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number Actions Reply - message the submitter/ PI/ all watchers Comment - attach internal message (no submitters, only Data Teamers) Open It - Open the ticket Stall - submitter has not responded in greater than 1 month Resolve - ticket completed History - message history and option to reply (to submitter and beyond) or comment (internal message) "],
["nesting-a-data-package.html", "Chapter 9 Nesting a data package 9.1 Introduction 9.2 Add children to an existing parent 9.3 Create a new parent package 9.4 Creating a new Parent package example", " Chapter 9 Nesting a data package 9.1 Introduction Data packages on member nodes can exist as independent packages or in groups (nested data packages). Much like we can group multiple data files together with a common metadata file, we can group related data packages together with a common “parent” data package. The structure of nested data packages resembles a pyramid. There is one top level, or “parent”, with one or more data packages, or “childs”, nested beneath it. There is no limit to how many nested levels can be created, but packages do not generally exceed 3 levels. This “grandparent” has has 5+ childs (nested datasets), all of which have child packages of their own. Here are some common uses for nesting: collected data vary by year an NSF award funds several related projects data collection is still ongoing data files exceed the 1000 file limit per data package 9.2 Add children to an existing parent A new package is published with a DOI and needs to be nested underneath a pre-existing parent. Nest the new child using the child_pids argument in publish_update(). resource_map_child_new &lt;- &quot;some_child_resource_map_pid&quot; pkg_parent &lt;- get_package(mn, &#39;resource_map_parent&#39;) publish_update(mn, resource_map_pid = pkg_parent$resource_map, metadata_pid = pkg_parent$metadata, data_pids = pkg_parent$data_pids, # parents usually don&#39;t contain data, but better to be safe child_pids = c(pkg_parent$child_packages, resource_map_child_new)) # include the resource map PIDs of ALL the childs* in the `child_pids` argument, otherwise the nesting relationships between any omitted childs and the parent will be deleted Check through all arguments carefully before you publish to production! Do you need to update the metadata? Does the parent include data objects? Does the parent have a parent? Parents can be tricky to fix and work with (especially if they have serial identifiers (SIDs)), so if you’re not sure how something works, try it on a test node. 9.3 Create a new parent package In some cases, a parent package already exists. Search the ADC for the NSF award number to see if there are already exisiting packages. Parents usually have a UUID rather than a DOI and often start with a title like “Collaborative research:”, but not always. More typically, you will need to create a new parent by editing the existing metadata. The parent package should contain a generalized summary for the metadata of each of its childs. To create a new parent, you will need to: Create parent metadata. It’s often easiest to start with a child’s metadata and generalize them. Abstract/title: Remove dates and other details that are specific to the child package. Sometimes the NSF award abstract/ title will work. Data tables/other entities: Generally, top-level parents do not include data objects, so these sections can be removed. Geographic coverage: Expand to include geographic coverage of all childs, if needed. Temporal coverage: Expand to include temporal ranges of all childs, if needed. If the study is ongoing, include the most recent end date; the parent can be updated when additional childs are added. Methods: Often not needed, but may be included if all childs use the same methods. Publish the parent metadata to the member node (ADC) using publish_object. Create a resource map to link the parent and childs together using create_resource_map and the child_pids argument. Make sure you use the childs’ resource map PIDs when you create the resource map! If you forgot to do so, consult Dom, Jesse, or Jeanette for help fixing it. 9.4 Creating a new Parent package example We can start by creating two data packages on the test node to nest beneath a parent. These data packages contain measurements taken from Lake E1 in Alaska in 2013 and 2014. First, load the Arctic Data Center Test Node and libraries. library(dataone) library(arcticdatautils) library(EML) cn_staging &lt;- CNode(&#39;STAGING&#39;) adc_test &lt;- getMNode(cn_staging,&#39;urn:node:mnTestARCTIC&#39;) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn, &#39;urn:node:ARCTIC&#39;) We will re-create the following parent package: https://arcticdata.io/catalog/#view/urn:uuid:799b7a86-cb1c-497c-a05a-d73492915cad on the test node with two of its children. First we will copy two of the children to the test node, make sure your token for the test node is not expired. from &lt;- dataone::D1Client(&quot;PROD&quot;, &quot;urn:node:ARCTIC&quot;) to &lt;- dataone::D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) child_pkg_1 &lt;- datamgmt::clone_package(&#39;resource_map_doi:10.18739/A2KS1R&#39;, from = from, to = to, add_access_to = arcticdatautils:::get_token_subject(), change_auth_node = TRUE, new_pid = TRUE) child_pkg_2 &lt;- datamgmt::clone_package(&#39;resource_map_doi:10.18739/A2QK29&#39;, from = from, to = to, add_access_to = arcticdatautils:::get_token_subject(), change_auth_node = TRUE, new_pid = TRUE) These two packages correspond to data from the same study, varying only by year; however, they currently exist on the test node as independent entities. We will associate them with each other by nesting them underneath a parent. Now, let’s create a parent metadata file. Read in one of the childs’ metadata files (EML). We can download object from a node in binary format using dataone::getObject. Once it’s downloaded we just need to convert to it to the proper format: in this case to EML format using EML::read_eml. eml_parent &lt;- read_eml(getObject(adc_test, child_pkg_1$metadata)) ## View the title eml_parent@dataset@title ## An object of class &quot;ListOftitle&quot; ## [[1]] ## &lt;title&gt;Time series of water temperature, specific conductance and oxygen from Lake E1, North Slope, Alaska, 2012-2013.&lt;/title&gt; The title of this child contains “2012-2013”. This is too specific for the parent, as the temporal range of both childs is 2012-2014. The parent should encompass this larger time range. eml_parent@dataset@title &lt;- read_eml(&#39;&lt;title&gt;Time series of water temperature, specific conductance, and oxygen from Lake E1, North Slope, Alaska, 2012-2014&lt;/title&gt;&#39;) Like the title, the temporal coverage elements in this EML need to be adjusted. new_end_date &lt;- new(&quot;calendarDate&quot;, &quot;2014-09-20&quot;) eml_parent@dataset@coverage@temporalCoverage@.Data[[1]]@rangeOfDates@endDate@calendarDate &lt;- new_end_date Remove dataTables and otherEntitys from the metadata. If you recall from previous chapters, dataTables contain metadata associated with data files (generally CSVs) and otherEntitys contain metadata about any other files in the data package (for instance a README or coding script). Because the parent does not contain any data objects, we want to remove dataTables and otherEntitys from the metdata file. In this instance, the E1 2013 metadata only contain dataTables. We can remove these by setting the dataTable element in the EML to a new blank object. eml_parent@dataset@dataTable &lt;- new(&quot;ListOfdataTable&quot;) In this case, the abstract, contacts, creators, geographicDescription, and methods are already generalized and do not require changes. Before writing your parent EML make sure that it validates. This is just a check to make sure everything is in the correct format. eml_validate(eml_parent) After your EML validates we need to save, or “write”, it as a new file. Write your parent EML to a directory in your home folder. You can view this process like using “Save as” in Microsoft Word. We opened a file (“E1_2013.xml”), made some changes, and “saved it as” a new file called “eml_parent.xml”. # We can save the eml in a temporary file eml_path &lt;- file.path(tempdir(), &#39;science_metadata.xml&#39;) write_eml(eml_parent, path) Next, we will publish the parent metadata to the test node. metadata_parent &lt;- publish_object(adc_test, path = eml_path, format_id = format_eml()) Finally, we create a resource map for the parent package. We nest the two child data packages using the child_pids argument in create_resource_map. Note that these child_pids are PIDs for the resource maps of the child packages, NOT the metadata PIDs. resource_map_parent &lt;- create_resource_map(adc_test, metadata_pid = metadata_parent, child_pids = c(child_pkg_1$resource_map, child_pkg_2$resource_map)) The child packages are now nested underneath the parent. "],
["building-provenance.html", "Chapter 10 Building Provenance 10.1 Introduction 10.2 The Prov Editor 10.3 Understanding resource maps 10.4 datapack 10.5 References", " Chapter 10 Building Provenance 10.1 Introduction The provenance chain describes the origin and processing history of data. Provenance (or “prov”) can exist on a continuum, ranging from prose descriptions of the history, to formal provenance traces, to fully executable environments. In this section we will describe how to build provenance using formal provenance traces in DataONE. Provenance is becoming increasingly important in the face of what is being called a reproducibility crisis in science. J. P. A. Ioannidis (2005) wrote that “Most Research Findings Are False for Most Research Designs and for Most Fields”. Ioannidis outlined ways in which the research process has lead to inflated effect sizes and hypothesis tests that codify existing biases. The first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. And yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). At NCEAS and in the datateam, not only do we archive data and code openly, but we also describe the workflows that involve that data and code using provenance, formalizing the provenance trace for a workflow that might look like this into an easily understandable trace including archived data objects, such as what is shown here. There are two ways that we add provenance in the datateam - the prov editor and the R datapack package. 10.2 The Prov Editor Provenance can easily be added to production Arctic Data Center packages using the provenance editor on beta.arcticdata.io. On the landing page of a data package within beta, in the dataTable or otherEntity section where you would like to add a provenance relationship, you can choose to add either a “source” or a “derivation”, to the left or right of the object pane, respectively. To add a source data file, click on the circle with the “+ add” text. Similarly, a source script would be added by selecting the arrow. Selecting the circle to add a source file pulls up the following screen, where you can select the source from other data objects within the same data package. A data package with an object that has multiple sources added will look like this. For simple packages on the Arctic Data Center, adding prov through the prov editor at beta.arcticdata.io is super easy! 10.3 Understanding resource maps Before we dive further into constructing prov in R, we need to talk more about resource maps (or “resmaps”). All data packages have a single resource map. But what is a resource map and how do we use one to find out what objects are in a particular data package? This document is a short introduction but a more complete guide can be found here. A resource map is a special kind of XML document that describes (among other things) an “aggregation”. The aggregation describes the members of a data package (metadata and data, usually). We can use the dataone R package to download a resource map if we know its PID: library(dataone) mnT &lt;- MNode(&quot;https://test.arcticdata.io/metacat/d1/mn/v2&quot;) pid &lt;- &quot;urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot; # A resource map PID path &lt;- tempfile(fileext = &quot;.xml&quot;) # We&#39;re saving to a temporary file but you can save elsewhere writeLines(rawToChar(getObject(mnT, pid)), path) # Write the object to `path` If we open that file up in a text editor, we see this: &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;rdf:RDF xmlns:cito=&quot;http://purl.org/spar/cito/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:foaf=&quot;http://xmlns.com/foaf/0.1/&quot; xmlns:ore=&quot;http://www.openarchives.org/ore/terms/&quot; xmlns:prov=&quot;http://www.w3.org/ns/prov#&quot; xmlns:provone=&quot;http://purl.dataone.org/provone/2015/01/15/ontology#&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot; xmlns:rdfs=&quot;http://www.w3.org/2000/01/rdf-schema#&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema#&quot;&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;cito:isDocumentedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;rdf:type rdf:resource=&quot;http://www.openarchives.org/ore/terms/Aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:isDocumentedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:documents rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:documents rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;ore:isAggregatedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;dc:title&gt;DataONE Aggregation&lt;/dc:title&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;ore:describes rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;ore:isAggregatedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;rdf:type rdf:resource=&quot;http://www.openarchives.org/ore/terms/ResourceMap&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;/rdf:RDF&gt; Whoa! What is this thing and how do you read it to find the members of the data package? The short answer is to look for lines like this: &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; This line says “The aggregation aggregates urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5” so that means urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5 is in our data package! The key bit is the &lt;rdf:Description rdf:about=&quot;...#aggregation part. If you look for another similar statement, you’ll also see that urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27 is part of our data package. Now we know which objects are in our data package but we don’t know which one contains metadata and which one contains data. For that, we need to get a copy of the system metadata for each object: getSystemMetadata(mnT, &quot;urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5&quot;)@formatId getSystemMetadata(mnT, &quot;urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;)@formatId From the formatIds, we can see the first PID is the EML (formatId: [1]&quot;eml://ecoinformatics.org/eml-2.1.1&quot;) and the second PID is a data object (formatId: [1]&quot;application/octet-stream&quot;). Now we know enough to know what’s in the data package: Resource map: urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556 Metadata: urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5 Data: urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27 Now that you’ve actually seen a resource map, we can dive further into prov. 10.4 datapack For packages not on the ADC, or packages that are extremely complicated, it may be best to upload prov relationships using R. The datapack package has several functions which help add relationships in a very simple way. These relationships are stored in the resource map. When you update a package to only add prov, the package will not be assigned any new identifiers with the exception of the resource map. First, we set the environment, in a similar, but slightly different way than what you may be used to. Here the function D1Client sets the DataONE client with the coordinating node instance as the first argument, and membernode as the second argument. library(dataone) library(datapack) d1c &lt;- D1Client(&quot;STAGING2&quot;, &quot;urn:node:mnTestKNB&quot;) Next, get the PID of the resource map of the data package you are adding prov to, and load that package into R using the getDataPackage function. resmapId &lt;- &quot;urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b&quot; pkg &lt;- getDataPackage(d1c, id=resmapId, lazyLoad=TRUE, limit=&quot;0MB&quot;, quiet=FALSE) Printing pkg in your console shows you the contents of the data package, including all of the objects and their names: &gt; pkg Members: filename format mediaType size identifier modified local esc...er.R application/R NA 888 knb.92049.1 n n PWS....csv text/csv NA 1871469 knb.92050.1 n n PWS....csv text/csv NA 1508128 knb.92051.1 n n NA eml:/...-2.1.1 NA 15658 urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b n y Package identifier: resource_map_urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b RightsHolder: http://orcid.org/0000-0002-2192-403X It will also show the existing relationships in the resource map, which in this case are mostly the “documents” relationships that specify that the metadata record is describing all of these data files. Relationships: subject predicate object 2 esc_reformatting_PWSweirTower.R cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 4 PWS_weirTower.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 1 PWS_Weir_Tower_export.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 3 urn:uuid:8f501606-...4-b22d-050a4176a97b dcterms:creator _r1515542097r415842r1 5 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents esc_reformatting_PWSweirTower.R 6 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents PWS_weirTower.csv 7 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents PWS_Weir_Tower_export.csv 8 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents urn:uuid:8f501606-...4-b22d-050a4176a97b 9 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b In this example above, the data package has two .csv files, with an R script that converts one to the other. To create our provenance trace, first we need to select the source object, and save the PID to a variable. We do this using the selectMember function, and we can query part of the system metadata to select the file that we want. This function takes the data package (pkg), the name of the sysmeta field to query (in this case we use the fileName), and the value that you want to match that field to (in this case, ‘PWS_Weir_Tower_export.csv’). sourceObjId &lt;- selectMember(pkg, name=&quot;sysmeta@fileName&quot;, value=&#39;PWS_Weir_Tower_export.csv&#39;) This returns a list of the source object PIDs that match the query (in this case only one object matches). &gt; sourceObjId [1] &quot;knb.92051.1&quot; Now we need to select our output object. Here, we use the selectMember function again, and save the result to a new variable. outputObjId &lt;- selectMember(pkg, name=&quot;sysmeta@fileName&quot;, value=&#39;PWS_weirTower.csv&#39;) Now we query for the R script. In this case, we query based on the value of the formatId as opposed to the fileName. This can be useful if you wish to select a large list of PIDs that are all similar. programObjId &lt;- selectMember(pkg, name=&quot;sysmeta@formatId&quot;, value=&quot;application/R&quot;) Next, you use these lists of PIDs and a function called describeWorkflow to add these relationships to the data package. Note that if you do not have a program in the workflow, or a source file, you can simply leave those arguments blank. pkg &lt;- describeWorkflow(pkg, sources=sourceObjId, program=programObjId, derivations=outputObjId) Viewing pkg again confirms that these relationships have been inserted into the data package, as shown by the wasDerivedFrom and wasGeneratedBy statements. It is always a good idea to print pkg to confirm that your PID selection process worked as expected, and your prov relationships make sense. Relationships (updated): subject predicate object 15 _1db49d06-ae98-4...9101-39f7c0b45a95 rdf:type prov:Association 14 _1db49d06-ae98-4...9101-39f7c0b45a95 prov:hadPlan esc_reformatting_PWSweirTower.R 1 esc_reformatting_PWSweirTower.R cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 16 esc_reformatting_PWSweirTower.R rdf:type provone:Program 8 PWS_weirTower.csv cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 11 PWS_weirTower.csv rdf:type provone:Data 20 PWS_weirTower.csv prov:wasDerivedFrom PWS_Weir_Tower_export.csv 19 PWS_weirTower.csv prov:wasGeneratedBy urn:uuid:3dd59b0...bc38-3b5d8fa644ac 6 PWS_Weir_Tower_export.csv cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 10 PWS_Weir_Tower_export.csv rdf:type provone:Data 9 _r1515544826r415842r1 foaf:name DataONE R Client 17 urn:uuid:3dd59b0...bc38-3b5d8fa644ac dcterms:identifier urn:uuid:3dd59b0...bc38-3b5d8fa644ac 13 urn:uuid:3dd59b0...bc38-3b5d8fa644ac rdf:type provone:Execution 12 urn:uuid:3dd59b0...bc38-3b5d8fa644ac prov:qualifiedAssociation _1db49d06-ae98-4...9101-39f7c0b45a95 18 urn:uuid:3dd59b0...bc38-3b5d8fa644ac prov:used PWS_Weir_Tower_export.csv 5 urn:uuid:8f50160...b22d-050a4176a97b cito:documents esc_reformatting_PWSweirTower.R 4 urn:uuid:8f50160...b22d-050a4176a97b cito:documents PWS_weirTower.csv 3 urn:uuid:8f50160...b22d-050a4176a97b cito:documents PWS_Weir_Tower_export.csv 2 urn:uuid:8f50160...b22d-050a4176a97b cito:documents urn:uuid:8f50160...b22d-050a4176a97b 7 urn:uuid:8f50160...b22d-050a4176a97b cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b Finally, you can upload the data package using the uploadDataPackage function, which takes the DataONE client d1c we set in the beginning, the updated pkg variable, some options for public read, and whether or not informational messages are printed during the upload process. First, you will need to run a token obtained from https://dev.nceas.ucsb.edu/ to publish to that node. resmapId_new &lt;- uploadDataPackage(d1c, pkg, public=TRUE, quiet=FALSE) If successful you should be able to navigate to the landing page of your dataset, and icons should show up where the sources and derivations are, such as in this example: 10.4.1 Fixing mistakes If you messed up updating a data package using datapack, there unfortunately isn’t a great way to undo your work, as the describeWorkflow only adds prov relationships, it does not replace them. If you messed up, the best course of action is to update the resource map with a clean version that does not have prov using update_resource_map, and then go through the steps outlined above again. Note: this has not been thorougly tested, and more extreme actions may be necessary to fully nuke the prov relationships. See Jeanette or Peter if things do not work as expected. 10.5 References Ioannidis, John P A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124. Hampton, Stephanie E, Sean Anderson, Sarah C Bagby, Corinna Gries, Xueying Han, Edmund Hart, Matthew B Jones, et al. 2015. “The Tao of Open Science for Ecology.” Ecosphere 6 (July). https://doi.org/10.1890/ES14-00402.1. Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. https://doi.org/10.1038/s41562-016-0021. "]
]
