[
["index.html", "Datateam miscellaneous ", " Datateam miscellaneous "],
["final-checklist-before-notifying-submitter-pi.html", "Final checklist before notifying submitter (PI)", " Final checklist before notifying submitter (PI) Descriptive title: what, where, when - provides enough information to understand the contents at a general scientific level (please use sentence case so that only the first word and proper nouns are capitalized, as in a journal article title; do not add a period at the end, it will auto-populate). Be sure to fully spell out all terms and avoid abbreviations, initialisms, or acronyms unless they are only generally known as such. Ex: “Active layer soil bulk density, moisture, carbon and nitrogen concentrations, and stable isotope data from borehole sites, Toolik Lake Field Station, 2015” Descriptive abstract: specifies purpose, data collected, and other applicable information (≥100 words) - provides an overview of the scientific context/project/hypotheses, how the data set fits into the larger project, a synopsis of the experimental or sampling design, and a summary of the data contents Bad example: - A low-resolution 3D model of a subglacial conduit derived from a photographic survey and structure-from-motion. Good example: - In this study, the locations of 193 old aerial photographs of northern Alaskan landscapes were re-photographed and assessed for changes in vegetation. The original photographs were taken over northern Alaska between 1948 and 1951, and the new photographs were taken between 1999 and 2003. The region covered by the original and repeat photographs stretches from the southern extent of the Brooks Range in the south to the Coastal Plain in the north, and from the Chukchi Sea in the west to the Canning River in the east. The original photographs were taken by the U.S. military as part of geologic reconnaissance and exploration, and the method used to acquire them was to fly both sides of a river valley while photographing the river and the facing valley slopes. Of the several thousand original photographs, only a fraction were repeated for the purpose of assessing vegetation change. Repeat photographs were selected for geographic coverage and to produce the greatest likelihood of detecting vegetation change. The original and repeat photographs were then scanned and stored in TIFF format. Individual image file sizes range from 5 to 60 MB each, and the total file size for the data set is approximately 11 GB. Data file types are registered DataONE formats and properly set in the EML (formatName) and the system metadata for ALL DATA OBJECTS See registered DataOne formats here Data files stored using proprietary software could disappear or no longer be accessible when the software is no longer available. Preserving data in text-based, open-source file formats ensures preservation of data for, theoretically, all time (ex: CSV instead of XLS). These transferrable file formats also reduce the possibility of information being “lost in translation”. fileName is properly set (including extension) in the system metadata for all objects (besides resource map) - do this by using set_file_name() Data file names: clear, but short descriptions without blank spaces (include extension) Ex: “Corvallis_VegBiodiv_2007.csv” Column headers (within data files): no spaces (use underscore or camelCase), attributeList adequately defines ALL column headers (including units) Cell etiquette: only one value (piece of information) per cell (for lat/long use decimal degrees) - attributeList should define ALL codes used (including defining missing value codes; WHY are data absent?) Entity level metadata: Each data file has either a dataTable or an otherEntity entityNames are properly set in all dataTables and otherEntitys Download buttons in dataTables and otherEntitys download correct data files dataTables and otherEntitys must include physical - all physical sections match the information for the respective data object: objectName (including extension), size (including units), authentication (checksum), formatName (file format), and url (online distribution info links end in correct PID) Any dimensionless units entered through the editor must be checked and quality controlled for accuracy. For attributes with numericDomains, the editor has a unit option of “Other / None” which populates as dimensionless in the EML. There is also a dimensionless option for submitters. While some attributes may be correctly marked as dimensionless (percent, ppm, ppb, other ratios, etc.), others may just be a placeholder in need of a custom unit. Please refer to here for more information. Creator and contact roles are complete - First/Last Name, emails are essential - use ONE givenName slot for EACH first OR middle name, like so: &lt;individualName&gt; &lt;givenName&gt;Austin&lt;/givenName&gt; &lt;givenName&gt;Samuel&lt;/givenName&gt; &lt;surName&gt;Post&lt;/surName&gt; &lt;/individualName&gt; - Whenever they exist and are known, ORCID iDs should be included for all party types - enter iDs (in this format: &lt;userId directory=&quot;https://orcid.org/&quot;&gt;https://orcid.org/0000-0000-0000-0000&lt;/userId&gt;) in the userId slots. Note the “s” in “https”. See how to do this in an automated fashion here - Use the references tag to copy information for the same individual to multiple party types (creator, contact, associatedParty), like so - Emails are important, phone #s and addresses are less critical Geographic and temporal ranges are sensible (start date preceeds end date) and appropriate to the study site and period; geographic description is accurate and includes commonly understood context Bad examples: - &quot;Svalbard&quot; - &quot;Sag River&quot; Good examples: - &quot;Svalbard, Norway&quot; - &quot;Sagavanirktok River, North Slope, Alaska&quot; Funding numbers are accurate and included NSF-funded projects can be searched here; we also accept non-NSF funded arctic data sets Methods: provide enough detail so that a reasonable scientist can interpret the study and data for reuse without needing to consult anyone nor any other resources Ensure that the project section of the EML matches the information from the NSF award as opposed to only being specific to this one data package Double check for garbled symbols, spelling, and gramatical errors If the data set is still private and awaiting PI and/or submitter review, be sure that the accessPolicys on ALL OBJECTS (metadata, resource map, data objects) grants read, write, and changePermission privileges to the PI’s and/or submitter’s ORCID iDs (in this format: “http://orcid.org/XXXX-XXXX-XXXX-XXXX”) - do this using set_access(). Make sure to OMIT the “s” after “http” in the ORCiD for this sysmeta field. This is different than how we want ORCiDs formatted in EMLs rightsHolder is properly set (in this format: “http://orcid.org/XXXX-XXXX-XXXX-XXXX”) to PIs on ALL OBJECTS (metadata, resource map, data objects) - do this using set_rights_holder() - this should be a last step After approval and DOI’ing, log out to ensure the landing page looks correct to average users. If there are discrepencies between the landing pages when you are logged in versus logged out, ensure all versions (of all objects) following a public version, are also public. "],
["cyberduck-instructions.html", "Cyberduck instructions", " Cyberduck instructions To use Cyberduck to transfer local files onto the Datateam server: Open Cyberduck. Check that you have the latest version (Cyberduck -&gt; Check for Update…). If not, download and install the latest (you may need Dom, Jesse, or Jeanette to enter a password). Click “Open Connection”. From the drop-down, choose “SFTP (Secure File Transfer Protocol)”. Enter “datateam.nceas.ucsb.edu” for Server. Enter your username and password. Connect. From here, you can drag and drop files to and from the server. "],
["edit-data-packages.html", "Edit data packages ", " Edit data packages "],
["add-a-pre-generated-identifier-to-the-eml.html", "Add a pre-generated identifier to the EML", " Add a pre-generated identifier to the EML When you pre-generate a UUID or DOI, the change is not automatically reflected in the packageId section of the EML. Use the code below to ensure that the EML lines up with the desired identifier: ## Generate DOI and add to EML # Note that you cannot generate a DOI on test nodes doiPid &lt;- generateIdentifier(mn, &quot;DOI&quot;) eml@packageId &lt;- new(&quot;xml_attribute&quot;, .Data = doiPid) Be sure to include the identifier= argument in your publish update command so the pre-generated identifier is applied. "],
["create-a-resource-map.html", "Create a resource map", " Create a resource map If you are creating a new data package, you must create a resource map. Resource maps provide information about the resources in the data package (e.g. what data files should be included in the package, where the metadata are, etc.). To create a new resource map with an existing published metadata_pid and data_pids, use the following command: resource_map_pid &lt;- create_resource_map(adc_test, metadata_pid = metadata_pid, data_pids = data_pids) "],
["edit-sysmeta.html", "Edit sysmeta", " Edit sysmeta To edit the sysmeta of an object (data file, EML, or resource map, etc.) with a PID, first load the sysmeta into R using the following command: sysmeta &lt;- getSystemMetadata(mn, pid) Then edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command: sysmeta@fileName &lt;- &#39;NewFileName.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access, which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata to change either the formatId or fileName slots (see the DataONE list of format ids) for acceptable formats. # Example of setting the formatId slot sysmeta@formatId &lt;- &quot;eml://ecoinformatics.org/eml-2.1.1&quot; After you have changed the necessary slot, you can update the system metadata using the following command: updateSystemMetadata(mn, pid, sysmeta) Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata are accurate when an object is first published). Additional resources For a more in-depth (and technical) guide to sysmeta, check out the DataONE documentation: System Metadata Data Types in CICore "],
["get-package-and-eml.html", "Get package and EML", " Get package and EML Before we look more in depth at EML, we first need to load your data package into R. After setting your node, use the following commands to load the package: rm_pid &lt;- &quot;your_resource_map_pid&quot; pkg &lt;- get_package(adc_test, rm_pid, file_names = TRUE) After loading the package, you can also load the EML file into R using the following command: eml &lt;- read_eml(getObject(adc_test, pkg$metadata)) Tip to always have the most recent resource map. When editing data packages, you always want to be working with the most recent update. To ensure you have the most recent resource map, you can use the following commands: rm_pid_original &lt;- &quot;your_original_resource_map_pid&quot; all_rm_versions &lt;- get_all_versions(adc_test, rm_pid_original) rm_pid &lt;- all_rm_versions[length(all_rm_versions)] print(rm_pid) "],
["obsolescence-chain.html", "Obsolescence chain", " Obsolescence chain You can obosolete a dataset using the function datamgmt::obsolete_package. Use the documentation for instructions on using the function. The following workflow explains how the functions operates: This chunk is to obsolete one data set. If there are more to add to the chain, more steps can be added. Be very careful. Make sure to fill in obsoletes and obsoletedBy slots for each one. The obsoletes and obsoletedBy fields must be NA, once they are populated they can’t be modified. # get oldest version of the file you want to be visible. Use get_all_versions and look at the latest. # urn:uuid:... # PID for data set to be obsoleted (hidden): doi:10… # adding data set to obsolete (hide) in the slot before the first version of the visible data set sysmeta1 &lt;- getSystemMetadata(mn, &quot;urn:uuid:example_pid&quot;) sysmeta1@obsoletes &lt;- &quot;doi:10.example_doi&quot; updateSystemMetadata(mn, &quot;urn:uuid:example_pid&quot;, sysmeta1) # adding first version to obsolescence chain after obsoleted (hidden) version sysmeta0 &lt;- getSystemMetadata(mn, &quot;doi:10.example_doi&quot;) sysmeta0@obsoletedBy &lt;- &quot;urn:uuid:example_pid&quot; updateSystemMetadata(mn, &quot;doi:10.example_doi&quot;, sysmeta0) The following code is equivalent to the code chunk above. This method is recommended, however it is necessary to read the function documenation first. datamgmt::obsolete_package(mn, metadata_obsolete = &quot;doi:10.example_doi&quot;, metadata_new = &quot;urn:uuid:example_pid&quot;) "],
["publish-an-object.html", "Publish an object", " Publish an object Objects (data files, xml metadata files) can be published to a DataONE node using the function publish_object from the arcticdatautils R Package. To publish an object, you must first get the formatId of the object you want to publish. A few common formatIds are listed below. # .csv file formatId &lt;- &quot;text/csv&quot; # .txt file formatId &lt;- &quot;text/plain&quot; # metadata file formatId &lt;- &quot;eml://ecoinformatics.org/eml-2.1.1&quot; # OR formatId &lt;- format_eml() Most objects have registered formatIds that can be found on the DataONE website here. Metadata files (as shown above) use a special function to set the formatId. If the formatId is not listed at the DataONE website, you can set formatId &lt;- &quot;application/octet-stream&quot;. Once you know the formatId you can publish your object using these commands: path &lt;- &quot;path/to/your/file&quot; formatId &lt;- &quot;your/formatId&quot; pid &lt;- publish_object(adc_test, path = path, format_id = formatId) After publishing the object, the PID will need to be added to a resource map by updating or creating a resource map. Additionally, the rights and access for the object must be set. However, you only need to give other people rights and access to objects to objects that are not yours, for the training you don’t need to do this. "],
["set-dataone-nodes.html", "Set DataONE nodes", " Set DataONE nodes DataONE is a network of data repositories that is structured with coordinating nodes (CN) and member nodes (MN). The network tree looks something like this: At the top level is DataONE itself. Within DataONE there are several coordinating nodes, including nodes for both production material and testing material. Within these coordinating nodes are many member nodes, including ones for both the Arctic Data Center and the KNB. To set the environment in which you want to publish data, you need to set both the coordinating node and the member node. For example, if you are publishing to the Arctic Data Center test site, you would want to set the coordinating node to STAGING and the member node to mnTestArctic. A note on nodes - be very careful about what you publish on production nodes (PROD, or arcticdata.io). These nodes should NEVER be used to publish test or training data sets. The primary nodes we work on, and how to set them in R, are below: Staging (Test) nodes # ADC (test.arcticdata.io) cn_staging &lt;- CNode(&#39;STAGING&#39;) adc_test &lt;- getMNode(cn_staging,&#39;urn:node:mnTestARCTIC&#39;) # KNB (dev.nceas.ucsb.edu) cn_staging2 &lt;- CNode(&quot;STAGING2&quot;) knb_test &lt;- getMNode(cn_staging2, &quot;urn:node:mnTestKNB&quot;) Production nodes # ADC (arcticdata.io) cn &lt;- CNode(&#39;PROD&#39;) adc &lt;- getMNode(cn,&#39;urn:node:ARCTIC&#39;) # KNB (knb.ecoinformatics.org) knb &lt;- getMNode(cn, &quot;urn:node:KNB&quot;) # GOA goa &lt;- getMNode(cn, &quot;urn:node:GOA&quot;) # You can also use the datamgmt::guess_membernode function to set a member node # Note: this pid looks like a URL - it&#39;s really a unique identifier dryad &lt;- datamgmt::guess_member_node(&#39;https://doi.org/10.5061/dryad.k6gf1tf/15?ver=2018-09-18T03:54:10.492+00:00&#39;) More DataONE STAGING nodes can be found here. More DataONE PROD nodes can be found here. "],
["set-rights-and-access.html", "Set rights and access", " Set rights and access One final step when creating/updating packages is to make sure that the rights and access on all the objects that were uploaded are set correctly within the sysmeta. The function arcticdatautils::set_rights_and_access will set both, and arcticdatautils::set_access will just set access. There are two functions for this because a rightsHolder should always have access, but not all people who need access are rightsHolders. The rightsHolder of the data package is typically the submitter (if the data set is submitted through the web form (“editor”)), but if a data team member is publishing objects for a PI, the rightsHolder should be the main point of contact for the data set (i.e. the person who requested that we upload the data for them). To set the rights and access for all of the objects in a package, first get the ORCiD of the person to whom you are giving rights and access. You can set this manually, or grab it from one of the creators in an EML file. You can look up ORCID iDs here. # Manually set ORCiD subject &lt;- &#39;http://orcid.org/PUT0-YOUR-ORCD-HERE&#39; # Set ORCiD from EML creator subject &lt;- eml@dataset@creator[[1]]@userId[[1]]@.Data # As a convention we use `http:` instead of `https:` in our system metadata subject &lt;- sub(&quot;^https://&quot;, &quot;http://&quot;, subject) Note, when setting metadata, the ORCiD must start with http://. ORCiDs in EML should start with https://. The sub command above will change this formatting for you. Next, set the rights and access using the following command: set_rights_and_access(adc_test, pids = c(pkg$metadata, pkg$data, pkg$resource_map), subject = subject, permissions = c(&#39;read&#39;,&#39;write&#39;,&#39;changePermission&#39;)) If you ever need to remove/add public access to your package or object, you can use remove_public_read or set_public_read, respectively. remove_public_read(adc_test, c(pkg$metadata, pkg$data, pkg$resource_map)) "],
["show-indexing-status.html", "Show indexing status", " Show indexing status Sometimes it takes a while for the website to render with the updates you’ve made in R. To check whether a PID has been indexed yet, use: show_indexing_status(mn, pid) The status bar will either show 0% (not indexed) or 100% (should be online already). "],
["update-a-package-with-a-new-data-object.html", "Update a package with a new data object", " Update a package with a new data object Once you have updated the data objects and saved the metadata to a file, we can update the metadata and add the new pid to the resource map using publish_update. Make sure you have the package you want to update, loaded into R using get_package. Publish update Now we can update your data package to include the new data object. eml_path &lt;- &quot;path/to/your/saved/eml.xml&quot; pkg &lt;- get_package(adc_test, &quot;resource_map_pid&quot;) update &lt;- publish_update(adc_test, metadata_pid = pkg$metadata, resource_map_pid = pkg$resource_map, data_pids = c(pkg$data, id_new), metadata_path = eml_path, public = FALSE) If a package is ready to be public, you can change the public argument in the publish_update call to TRUE. If you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you can change the use_doi argument to TRUE. This should only be done after the package is finalized and has been thoroughly reviewed! If the package has children, set the child_pids argument to pkg$child_packages. Refresh the landing page at test.arcticdata.io/#view/… for this package and then follow the “newer version” link to view the latest. "],
["update-a-data-file.html", "Update a data file", " Update a data file To update a data file, you need to do three things: update the object itself, update the metadata that describes that object update the resource map (which affiliates the object with the metadata). The arcticdatautils::update_object function takes care of the first of these tasks. As arguments, update object takes the member node instance you are working in, the pid of the object to be updated, the path to the new version of that object on your computer, and the format id of the object. The code will look like this (very similar to the publish_object function call): id_new &lt;- update_object(adc_test, pid = &quot;the data pid you need to update&quot;, path = &quot;path/to/new/file.csv&quot;, format_id = &quot;text/csv&quot;) You will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Use line 2 (Id:) exactly, character for character. To acomplish the second task, you will need to update the metadata using the EML package. This is covered in Chapter 4. After you update a file, you will always need to update the metadata because parts of the physical section (such as the file size, checksum) will be different, and it may also require different attribute information. Once you have updated your metadata and saved it, you can update the package itself. "],
["explore-eml.html", "Explore EML ", " Explore EML "],
["access-specific-elements.html", "Access specific elements", " Access specific elements The eml_get function is a powerful tool for exploring EML (more on that here). It takes any chunk of EML and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples: eml &lt;- read_eml(system.file(&quot;example-eml.xml&quot;, package = &quot;arcticdatautils&quot;)) eml_get(eml, &quot;creator&quot;) [[1]] An object of class &quot;ListOfcreator&quot; [[1]] &lt;creator system=&quot;uuid&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Bryce&lt;/givenName&gt; &lt;surName&gt;Mecum&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;National Center for Ecological Analysis and Synthesis&lt;/organizationName&gt; &lt;/creator&gt; eml_get(eml, &quot;boundingCoordinates&quot;) &lt;boundingCoordinates&gt; &lt;westBoundingCoordinate&gt;-135&lt;/westBoundingCoordinate&gt; &lt;eastBoundingCoordinate&gt;-134&lt;/eastBoundingCoordinate&gt; &lt;northBoundingCoordinate&gt;59&lt;/northBoundingCoordinate&gt; &lt;southBoundingCoordinate&gt;57&lt;/southBoundingCoordinate&gt; &lt;/boundingCoordinates&gt; eml_get(eml, &quot;url&quot;) [1] &quot;ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456&quot; You can also use the which_in_eml function from the datamgmt package to get indices within an EML list. Here are some examples: # Question: Which creators have a surName &quot;Mecum&quot;? n &lt;- which_in_eml(eml@dataset@creator, &quot;surName&quot;, &quot;Mecum&quot;) # Answer: eml@dataset@creator[n] # Question: Which dataTables have an entityName that begins with &quot;2016&quot; n &lt;- which_in_eml(eml@dataset@dataTable, &quot;entityName&quot;, function(x) {grepl(&quot;^2016&quot;, x)}) # Answer: eml@dataset@dataTable[n] # Question: Which attributes in dataTable[[1]] have a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable[[1]]@attributeList@attribute, &quot;numberType&quot;, &quot;natural&quot;) # Answer: eml@dataset@dataTable[[1]]@attributeList@attribute[n] #&#39; # Question: Which dataTables have at least one attribute with a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable, &quot;numberType&quot;, function(x) {&quot;natural&quot; %in% x}) # Answer: eml@dataset@dataTable[n] "],
["navigate-through-eml.html", "Navigate through EML", " Navigate through EML The first task when editing an EML file is navigating the EML file. An EML file is organized in a structure that contains many lists nested within other lists. The function EML::eml_view (make sure to install and load the listviewer package if you don’t have it already) allows you to get a crude view of an EML file in the viewer. It can be useful for exploring the file. eml_raw &lt;- rawToChar(getObject(adc, &quot;doi:10.18739/A2FS1H&quot;)) eml_view(eml_raw) # you can also pass a path to an EML file To navigate this complex structure in R, use the @ symbol. The @ symbol allows you to go deeper into the EML structure and to see what slots are nested within other slots. However, you have to tell R where you want to go in the structure when you use the @ symbol. For example, if you want to go into the data set of your EML you would use the command eml@dataset. If you want to go to the creators of your data set you would use eml@dataset@creator. Note here that creators are contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing @ and a list of available locations in the structure will appear (e.g., eml@&lt;TAB&gt;): Note that if you hit tab, and the only option is .Data, this most likely implies that you are trying to go deeper within a list. For example eml@dataset@creator@&lt;TAB&gt; will return only .Data. This is because creator is a list object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is the index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now eml@dataset@creator[[1]]@&lt;TAB&gt; will give you many more options. Note, .Data sometimes means you have reached the end of a branch in the EML structure. At this point stop and take a deep breath. The key takeaway is that EML is a hierarchical tree structure. The best way to get familiar with it is to explore the structure. Try entering eml@dataset into your console, and print it. Now make the search more specific, for instance: eml@dataset@abstract. "],
["understand-the-eml-schema.html", "Understand the EML schema", " Understand the EML schema Another great resource for navigating the EML structure is looking at the schema which defines the structure. The .png files on this page show the schema as a diagram. Additional information on the schema and how different elements (or “slots”) are defined can be found here). Further explanations of the symbology can be found here. The schema is complicated and may take some time to get familiar with before you will be able to fully understand it. For example, let’s take a look at eml-party. To start off, notice that some elements are in solid boxes, whereas others are in dashed boxes. A solid box indicates that the element is required if the element above it (to the left in the schema) is used, whereas a dashed box indicates that the element is optional. Notice also that below the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname. You will also see icons linking the EML slots together, which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema. The eml schema sections you may find particularly helpful include eml-party, eml-attribute, and eml-physical. (Click “Download” to zoom in.) For a more detailed description of the EML schema, see the reference section on exploring EML. "],
["edit-eml.html", "Edit EML ", " Edit EML "],
["edit-an-eml-element.html", "Edit an EML element", " Edit an EML element There are multiple ways to edit an EML element. Edit EML with strings The most basic way to edit an EML element would be to go into the EML schema to the location of a character string and then replace that character string with a new one. For example, to change the title one could use the following commands: new_title &lt;- &quot;New Title&quot; eml@dataset@title[[1]]@.Data &lt;- new_title However, this isn’t the best method to edit the EML (unless you are an expert in both S4 objects and the EML schema) since the nesting and lists of elements can get very complex. Edit EML with the “EML” package For editing simple text sections, a better option is to use EML::read_eml. To use this function, one would first run the slot that is in need of editing. For example, for a title this would involve calling eml@dataset@title[[1]] and then copying the result. In this case, the result will be of the form &lt;title&gt;Title Text Here&lt;/title&gt;. To make a new title, one would replace the text between the &lt;title&gt;&lt;/title&gt; tags using a similar workflow as below. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title[[1]] &lt;- new_title Note, without specifying which title with [[1]], the following code will give you the error Error in (function (cl, name, valueClass) : assignment of an object of class “title” is not valid for @‘title’ in an object of class “dataset”; is(value, &quot;ListOftitle&quot;) is not TRUE. # Bad Example new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- new_title The above gives an error because eml@dataset@title is a slot for a list and new_title is a single object. Therefore you must either specify which title you want to replace as was done above by specifying the first title in the list with [[1]] or turn new_title into a list/vector utilizing the c() command as follows. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- c(new_title) However, if there were multiple titles, the above would replace all the titles with the single title. This behavior may or may not be desirable so be careful. One final note, a benefit of this method to edit EML objects is that advanced text features can be easily added using this workflow. Edit EML with objects A final way to edit an EML element would be to build a new object to replace the old object. To begin, you must determine the class of the object you want to edit (generally this is just the schema name of the object). The function class() is helpful here. For example, if you want to edit eml@dataset@title[[1]] use the following command to find the class: class(eml@dataset@title[[1]]) The result shows that this object has a class title. Therefore you must replace it with an object of class title. To do so, use as(). To use as(), input the desired string followed by the desired class. new_title &lt;- as(&quot;New Title 3&quot;, &quot;title&quot;) eml@dataset@title[[1]] &lt;- new_title #or eml@dataset@title &lt;- c(new_title) Note that if you want to create an object with nested objects, you may have to use the command new() which is similar to as() but with the order of specifying values and class switched. See help on editing datatables for an example of when to use new(). "],
["edit-attributelists.html", "Edit attributeLists", " Edit attributeLists Attributes are stored in an attributeList. When editing attributes in R, you need to create one to three objects: A data.frame of attributes A data.frame of custom units (if applicable) A data.frame of factors (if applicable) Attributes can exist in EML for dataTable, otherEntity, and spatialVector data objects. Please note that submitting attribute information through the website will store them in an otherEntity object by default. We prefer to store them in a dataTable object for tabular data or a spatialVector object for spatial data. To edit or examine an existing attribute table already in an EML file, you can use the following commands: # If they are stored in an otherEntity (submitted from the website by default) attributeList &lt;- EML::get_attributes(eml@dataset@otherEntity[[i]]@attributeList) # Or if they are stored in a dataTable (usually created by a datateam member) attributeList &lt;- EML::get_attributes(eml@dataset@dataTable[[i]]@attributeList) # Or if they are stored in a spatialVector (usually created by a datateam member) attributeList &lt;- EML::get_attributes(eml@dataset@spatialVector[[i]]@attributeList) attributes &lt;- attributeList$attributes print(attributes) Edit Attributes Attribute information should be stored in a data.frame with the following columns: attributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp” attributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)” attributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.” measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. e.g.: Low, Medium, High dateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001 ratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second. interval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or latitude and longitude coordinates, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius. domain: One of: textDomain, enumeratedDomain, numericDomain, dateTime. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTime, NA otherwise. Format string for dates, e.g. “DD/MM/YYYY”. definition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases. unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found here. numberType: Required for numericDomain, NA otherwise. Options are real, natural, whole, and integer. real: positive and negative fractions and integers (…-1,-0.25,0,0.25,1…) natural: non-zero positive integers (1,2,3…) whole: positive integers and zero (0,1,2,3…) integer: positive and negative integers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. You can create attributes manually by typing them out in R following a workflow similar to the one below: attributes &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Sample number&#39;, NA, NA, NA, &#39;comments about sampling process&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;), stringsAsFactors = FALSE) However, typing this out in R can be a major pain. Luckily, there is a Shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you), to edit an existing attribute table, or to create attributes from scratch. Use the following commands to create or modify attributes (these commands will launch a Shiny app in your web browser): # From data (recommended) datamgmt::create_attributes_table(data = data) # From an existing attribute table datamgmt::create_attributes_table(attributes_table = attributes_table) # From scratch datamgmt::create_attributes_table() Once you are done editing a table in the app, click the Print button to print text of a code that will build a data.frame in R. Copy that code and assign it to a variable in your script (e.g. attributes &lt;- data.frame(...)). For simple attribute corrections, datamgmt::edit_attribute allows you to edit the slots of a single attribute within an attribute list. Edit Custom Units EML has a set list of units that can be added to an EML file. These can be seen by using the following code: standardUnits &lt;- EML::get_unitList() View(standardUnits$units) If you have units that are not in the standard EML unit list, you will need to build a custom unit list. A unit typically consists of the following fields: id: The unit id (ids are camelCased) unitType: The unitType (run View(standardUnits$unitTypes) to see standard unitTypes) parentSI: The parentSI unit (e.g. for kilometer parentSI = “meter”) multiplierToSI: Multiplier to the parentSI unit (e.g. for kilometer multiplierToSI = 1000) name: Unit abbreviation (e.g. for kilometer name = “km”) description: Text defining the unit (e.g. for kilometer description = “1000 meters”) Additionally, datamgmt::create_attributes_table will tell you if each of your units are standard or not. If your unit is not standard, you should use the following code to help auto-generate a custom unit: datamgmt::return_eml_units(&quot;your_unit&quot;) Note that datamgmt::create_attributes_table calls datamgmt::return_eml_units for you! datamgmt::return_eml_units will auto-generate many of these fields for you (but don’t just assume the auto-generation will be perfect; always ensure the auto-generation correctly handles your unit.) Edit Factors For attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition. attributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute. code should contain all unique values of the given attributeName that exist within the actual data. definition should contain a plain text definition that describes each code. There is a tab in the datamgmt::create_attributes_table app that will help you build factors. If you need to build factors by hand, you can use named character vectors and then convert them to a data.frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region”. Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a data.frame using the names of the named character vectors and their definitions. factors &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) Finalize attributeList Once you have built your attributes, factors, and custom units, you can add them to EML objects. Attributes and factors are combined to form an attributeList using the following command: attributeList &lt;- EML::set_attributes(attributes = attributes, factors = factors) This attributeList must then be added to a dataTable. Custom units are added to additionalMetadata using the following command: unitlist &lt;- set_unitList(custom_units) eml@additionalMetadata &lt;- c(as(unitlist, &quot;additionalMetadata&quot;)) "],
["edit-datatables.html", "Edit dataTables", " Edit dataTables To edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable with the new() command as follows: dataTable &lt;- new(&quot;dataTable&quot;, entityName = &quot;A descriptive name for the data (does not need to be the same as the data file)&quot;, entityDescription = &quot;A description of the data&quot;, physical = physical, attributeList = attributeList) The dataTable must then be set to the EML (i.e.: eml@dataset@dataTable[[i]] &lt;- dataTable). "],
["edit-otherentities.html", "Edit otherEntities", " Edit otherEntities Remove otherEntities To remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable. eml@dataset@otherEntity[[i]] &lt;- NULL Create otherEntities If you need to create/update an otherEntity, make sure to publish or update your data object first (if it is not already on the DataONE MN). Then build your otherEntity. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, pkg$data[[i]]) Alternatively, you can build the otherEntity of a data object not in your package by simply inputting the data PID. otherEntity &lt;- arcticdatautils::pid_to_eml_entity(mn, &quot;your_data_pid&quot;, entityType = &quot;otherEntity&quot;, entityName = &quot;Entity Name&quot;, entityDescription = &quot;Description about entity&quot;) The otherEntity must then be set to the EML, like so: eml@dataset@otherEntity &lt;- otherEntity If you have additional otherEntity objects in the EML already, you will need to add the new one like this: eml@dataset@otherEntity[[i]] &lt;- otherEntity Where i is set to the number of existing entities plus one. "],
["edit-spatialvectors.html", "Edit spatialVectors", " Edit spatialVectors Occasionally, you may encounter a third type of data object: spatialVector. This object contains spatial data, such as a shapefile or geodatabase. Editing a spatialVector is similar to editing a dataTable or an otherEntity. A physical and attributeList should be present. One important difference is that a spatialVector object should also have a geometry slot that describes the geometry features of the data. The possible values include one or more (in a list) of ‘Point’, ‘LineString’, ‘LinearRing’, ‘Polygon’, ‘MultiPoint’, ‘MultiLineString’, ‘MultiPolygon’, or ‘MultiGeometry’. To add a geometry slot use: eml@dataset@spatialVector[[1]]@geometry[[1]] &lt;- read_eml(&quot;&lt;geometry&gt;Polygon&lt;/geometry&gt;&quot;) Additionally, spatial data should typically be archived within a .zip file to ensure all related and interdependent files stay together. For example, a spatial dataset for a shapefile should, at a minimum, consist of separate .dbf, .shp, and .shx files with the same prefix in the same directory. All these files are required in order to use the data. Also note that shapefiles limit attribute names to 10 characters, so attribute names in the metadata may not match exactly to attribute names in the data. Here is an example of what spatialVector metadata should look like, including physical, attributeList, and geometry slots: &lt;spatialVector system=&quot;uuid&quot;&gt; &lt;entityName&gt;sasap_regions.zip&lt;/entityName&gt; &lt;entityDescription&gt;Contains the shapefile depicting the SASAP regions. Zip contains .cpg, .dbf, .prj, .shp, and .shx files.&lt;/entityDescription&gt; &lt;physical scope=&quot;document&quot;&gt; &lt;objectName&gt;sasap_regions.zip&lt;/objectName&gt; &lt;size unit=&quot;bytes&quot;&gt;2533992&lt;/size&gt; &lt;authentication method=&quot;SHA1&quot;&gt;2d199f6f1f5f5b36525d1cf1019c0a4551b98762&lt;/authentication&gt; &lt;dataFormat&gt; &lt;externallyDefinedFormat&gt; &lt;formatName&gt;application/zip&lt;/formatName&gt; &lt;/externallyDefinedFormat&gt; &lt;/dataFormat&gt; &lt;distribution scope=&quot;document&quot;&gt; &lt;online&gt; &lt;url function=&quot;download&quot;&gt;https://cn.dataone.org/cn/v2/resolve/urn:uuid:f6ab206b-312c-4caf-89c8-89eb9d031aac&lt;/url&gt; &lt;/online&gt; &lt;/distribution&gt; &lt;/physical&gt; &lt;attributeList&gt; &lt;attribute&gt; &lt;attributeName&gt;region_id&lt;/attributeName&gt; &lt;attributeDefinition&gt;SASAP region ID&lt;/attributeDefinition&gt; &lt;measurementScale&gt; &lt;interval&gt; &lt;unit&gt; &lt;standardUnit&gt;dimensionless&lt;/standardUnit&gt; &lt;/unit&gt; &lt;numericDomain&gt; &lt;numberType&gt;natural&lt;/numberType&gt; &lt;/numericDomain&gt; &lt;/interval&gt; &lt;/measurementScale&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;attributeName&gt;region&lt;/attributeName&gt; &lt;attributeDefinition&gt;SASAP region name&lt;/attributeDefinition&gt; &lt;measurementScale&gt; &lt;nominal&gt; &lt;nonNumericDomain&gt; &lt;textDomain&gt; &lt;definition&gt;SASAP region name&lt;/definition&gt; &lt;/textDomain&gt; &lt;/nonNumericDomain&gt; &lt;/nominal&gt; &lt;/measurementScale&gt; &lt;/attribute&gt; &lt;/attributeList&gt; &lt;geometry&gt;Polygon&lt;/geometry&gt; &lt;/spatialVector&gt; "],
["format-text-in-eml.html", "Format text in EML", " Format text in EML Currently, only certain fields (abstracts, methods) support text formatting in EML. Check out this demo for a full example. Additional info is also available here. Also note, you will need to edit an EML section with EML in order to use these workflows. Many of these formatting functions only work when enclosed by &lt;para&gt;&lt;/para&gt; Type-setting Subscripts, superscripts, and italics: &lt;subscript&gt;You can do subscripts&lt;/subscript&gt; &lt;superscript&gt;or superscipts&lt;/superscript&gt; &lt;emphasis&gt;or even italics.&lt;/emphasis&gt; Links Be sure to incluse the “https://” before the link or it will redirect incorrectly. Also, always check that your links go through to the correct page. Please be aware that most links are inherently unstable so always default to archiving files over pointing to websites when possible and appropriate. &lt;ulink url=&quot;https://some_url.com&quot;&gt; &lt;citetitle&gt;some text&lt;/citetitle&gt; &lt;/ulink&gt; Lists Unordered (bulletted) lists: &lt;itemizedlist&gt; &lt;listitem&gt; &lt;para&gt;Paragraphs&lt;/para&gt; &lt;/listitem&gt; &lt;listitem&gt; &lt;para&gt;Sections w/ subsections (w/ titles)&lt;/para&gt; &lt;/listitem&gt; &lt;/itemizedlist&gt; Ordered lists (1, 2, 3)… &lt;orderedlist&gt; &lt;listitem&gt; &lt;para&gt;something&lt;/para&gt; &lt;/listitem&gt; &lt;listitem&gt; &lt;para&gt;something else&lt;/para&gt; &lt;/listitem&gt; &lt;/orderedlist&gt; "],
["set-coverages.html", "Set coverages", " Set coverages Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a data set. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering eml@dataset@coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage. coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;), geographicDescription = &quot;The geographic region covers the lake region near Eagle Mountain.&quot;, west = -154.6192, east = -154.5753, north = 68.3831, south = 68.3619) eml@dataset@coverage &lt;- coverage You can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages: geocov1 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 1&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 68), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -154), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 67), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -155))) geocov2 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 2&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 65), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -155), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 64), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -156))) coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;)) eml@dataset@coverage@geographicCoverage &lt;- c(geocov1, geocov2) To set taxonomic coverage: # add each new element as a tax object tax1 &lt;- new(&quot;taxonomicClassification&quot;, taxonRankName = new(&quot;taxonRankName&quot;, &quot;Species&quot;), taxonRankValue = new(&quot;taxonRankValue&quot;, &quot;Calamagrostis deschampsioides&quot;)) tax2 &lt;- new(&quot;taxonomicClassification&quot;, taxonRankName = new(&quot;taxonRankName&quot;, &quot;Species&quot;), taxonRankValue = new(&quot;taxonRankValue&quot;, &quot;Carex aquatilis&quot;)) # combine all tax elements into taxonomic coverage object taxcov &lt;- new(&quot;taxonomicCoverage&quot;, taxonomicClassification = c(tax1, tax2)) eml@dataset@coverage@taxonomicCoverage &lt;- c(taxcov) "],
["set-methods.html", "Set methods", " Set methods The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the new command. Remember you can explore possible slots within an element by creating an empty object of the class you are trying to create. For example, method_step &lt;- new('methodStep'), and using auto-complete on method_step@. Potentially the most useful way to set methods is by editing with the EML package. Another simple and potentially useful way to add methods to an EML that has no methods at all is by adding them via a MS Word document. An example is shown below: methods1 &lt;- set_methods(&#39;methods_doc.docx&#39;) eml@dataset@methods &lt;- methods1 If you want to make minor changes to existing methods information that has a lot of nested elements, your best bet may be to edit the EML manually in a text editor (or in RStudio), otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing methods information. Adding sampling info to methods section # add method steps as new variables step1 &lt;- new(&#39;methodStep&#39;, description = &quot;methods text&quot;) stEx &lt;- new(&quot;studyExtent&quot;, description = &quot;study extent description&quot;) samp &lt;- new(&quot;sampling&quot;, studyExtent = stEx, samplingDescription = &quot;sampling description text&quot;) # combine all methods steps and sampling info methods1 &lt;- new(&quot;methods&quot;, methodStep = c(step1), sampling = samp) eml@dataset@methods &lt;- methods1 "],
["set-parties.html", "Set parties", " Set parties To add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associatedParty classes. NCEASadd &lt;- new(&quot;address&quot;, deliveryPoint = &quot;735 State St #300&quot;, city = &quot;Santa Barbara&quot;, administrativeArea = &#39;CA&#39;, postalCode = &#39;93101&#39;) The creator, contact, and associatedParty classes can easily be created using functions from the arcticdatautils package. Here, we use eml_creator to set our data set creator. JC_creator &lt;- arcticdatautils::eml_creator(given_names = &quot;Jeanette&quot;, sur_name = &quot;Clark&quot;, organization = &quot;NCEAS&quot;, email = &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@creator &lt;- c(JC_creator) Similarly, we can set the contacts. In this case, there are two, so we set eml@dataset@contact as a ListOfcontact, which contains both of them. JC_contact &lt;- arcticdatautils::eml_contact(given_names = &quot;Jeanette&quot;, sur_name = &quot;Clark&quot;, organization = &quot;NCEAS&quot;, email = &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) JG_contact &lt;- arcticdatautils::eml_contact(given_names = &quot;Jesse&quot;, sur_name = &quot;Goldstein&quot;, organization = &quot;NCEAS&quot;, email = &quot;jgoldstein@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@contact &lt;- c(JC_contact, JG_contact) Finally, the associatedPartys are set. Note that associatedPartys MUST have a role defined, unlike creator or contact. JG_ap &lt;- arcticdatautils::eml_associated_party(given_names = &quot;Jesse&quot;, sur_name = &quot;Goldstein&quot;, organization = &quot;NCEAS&quot;, email = &quot;jgoldstein@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, address = NCEASadd, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, role = &quot;metaataProvider&quot;) eml@dataset@associatedParty &lt;- c(JG_ap) "],
["set-physical.html", "Set physical", " Set physical To set the physical aspects of a data object, use the following commands to build a physical object from a data PID that exists in your package. Remember to set the member node to test.arcticdata.io ! physical &lt;- arcticdatautils::pid_to_eml_physical(mn, pkg$data[[i]]) Alternatively, you can set the physical of a data object not yet in your package by simply inputting the data PID: physical &lt;- arcticdatautils::pid_to_eml_physical(mn, &quot;your_data_pid&quot;) The physical must then be assigned to the data object. A final, but not recommended option, is to set the physical by hand. To do so, one can use a workflow similar to the one below. However, the far superior workflow is to publish or update your data first and then use pid_to_eml_physical to set the physical. id &lt;- &#39;your_data_pid&#39; # this should be an actual PID path &lt;- &#39;~/your/data/path&#39; # path to data table physical &lt;- EML::set_physical(objectName = &#39;your_file_name&#39;, id = id, size = as.character(file.size(path)), sizeUnit = &#39;bytes&#39;, authentication = digest(path, algo=&quot;sha1&quot;, serialize=FALSE, file=TRUE), authMethod = &#39;SHA-1&#39;, numHeaderLines = &#39;1&#39;, fieldDelimiter = &#39;,&#39;, url = paste0(&#39;https://cn.dataone.org/cn/v2/resolve/&#39;, id)) "],
["use-references.html", "Use references", " Use references Introduction References are a way to avoid repeating the same information multiple times in the same EML record. There are a few benefits to doing this, including: Making it clear that two things are the same (e.g., the creator is the same person as the contact, two entities have the exact same attributes) Reducing the size on disk of EML records with highly redundant information Faster read/write/validate with the R EML package You may want to use EML references if you have the following scenarios (not exhaustive): One person has multiple roles in the dataset (creator, contact, etc) One or more entities shares all or some attributes Example with parties It’s very common to see the contact and creator referring to the same person with XML like this: ## &lt;eml packageId=&quot;my_test_doc&quot; system=&quot;my_system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1 eml.xsd&quot;&gt; ## &lt;dataset&gt; ## &lt;creator&gt; ## &lt;individualName&gt; ## &lt;givenName&gt;Bryce&lt;/givenName&gt; ## &lt;surName&gt;Mecum&lt;/surName&gt; ## &lt;/individualName&gt; ## &lt;/creator&gt; ## &lt;contact&gt; ## &lt;individualName&gt; ## &lt;givenName&gt;Bryce&lt;/givenName&gt; ## &lt;surName&gt;Mecum&lt;/surName&gt; ## &lt;/individualName&gt; ## &lt;/contact&gt; ## &lt;/dataset&gt; ## &lt;/eml&gt; So you see those two times Bryce Mecum is referenced there? If you mean to state that Bryce Mecum is the creator and contact for the dataset, this is a good start. But with just a name, there’s some ambiguity as to whether the creator and contact are truly the same person. Using references, we can remove all doubt. eml@dataset@creator[[1]]@id &lt;- new(&quot;xml_attribute&quot;, &quot;reference_id&quot;) eml@dataset@contact[[1]] &lt;- new(&quot;contact&quot;, reference = &quot;reference_id&quot;) print(eml) ## &lt;eml packageId=&quot;my_test_doc&quot; system=&quot;my_system&quot; xsi:schemaLocation=&quot;eml://ecoinformatics.org/eml-2.1.1 eml.xsd&quot;&gt; ## &lt;dataset&gt; ## &lt;creator id=&quot;reference_id&quot;&gt; ## &lt;individualName&gt; ## &lt;givenName&gt;Bryce&lt;/givenName&gt; ## &lt;surName&gt;Mecum&lt;/surName&gt; ## &lt;/individualName&gt; ## &lt;/creator&gt; ## &lt;contact&gt; ## &lt;references&gt;reference_id&lt;/references&gt; ## &lt;/contact&gt; ## &lt;/dataset&gt; ## &lt;/eml&gt; The reference id needs to be unique within the EML record but doesn’t need to have meaning outside of that. The above result can also be achieved by using arcticdatautils::eml_set_reference. eml@dataset@contact[[1]] &lt;- eml_set_reference(element_to_reference = eml@dataset@creator[[1]], element_to_replace = eml@dataset@contact[[1]]) Example with attributes To use references with attributes: Add an attribute list to a data table Add a reference id for that attribute list Use references to add that information into the attributeLists of the other data tables For example, if all the data tables in our data package have the same attributes, we can set the attribute list for the first one, and use references for the rest: eml@dataset@dataTable[[1]]@attributeList &lt;- attribute_list eml@dataset@dataTable[[1]]@attributeList@id &lt;- new(&quot;xml_attribute&quot;, &quot;shared_attributes&quot;) for (i in 2:length(eml@dataset@dataTable)) { ref &lt;- new(&quot;references&quot;, &quot;shared_attributes&quot;) eml@dataset@dataTable[[i]]@attributeList@references &lt;- ref } The above result can also be achieved by using arcticdatautils::eml_set_shared_attributes. This function will replicate an attributeList to all of the dataTable or otherEntity elements in an EML document. For more specific information consult the documentation. eml &lt;- eml_set_shared_attributes(eml, attributeList = attribute_list, type = &#39;dataTable&#39;) Add creator IDs The datamgmt::add_creator_id() function is a shortcut for one of our most common use-cases: you get a package that does not have an ORCID associated with any of the contacts, and you need to add at least one into the metadata. To do this, you can: Add the ORCID to a creator (usually the first creator). You can look up ORCIDs here. Update the contact information for that individual’s other roles eml &lt;- eml %&gt;% datamgmt::add_creator_id(surname = &quot;mecum&quot;, orcid = &quot;https://orcid.org/0000-1234-5678-4321&quot;, id = &quot;bryce&quot;) ## The following entry has been changed:&lt;creator id=&quot;bryce&quot; system=&quot;uuid&quot;&gt; ## &lt;individualName&gt; ## &lt;givenName&gt;Bryce&lt;/givenName&gt; ## &lt;surName&gt;Mecum&lt;/surName&gt; ## &lt;/individualName&gt; ## &lt;userId directory=&quot;https://orcid.org&quot;&gt;https://orcid.org/0000-1234-5678-4321&lt;/userId&gt; ## &lt;/creator&gt; # Use references to add updated contact info to Henrietta&#39;s other roles eml@dataset@contact[[1]] &lt;- new(&#39;contact&#39;, reference = &quot;bryce&quot;) eml@dataset@metadataProvider[[1]] &lt;- new(&#39;metadataProvider&#39;, reference = &quot;bryce&quot;) Updated creator information cannot be used as a reference for associatedParties because the extra “role” field is required. Also, the function does not (yet) account for cases in which multiple creators have the same surname. Check out the help file for add_creator_id for more information: ?datamgmt::add_creator_id "],
["pi-correspondence.html", "PI correspondence ", " PI correspondence "],
["email-templates.html", "Email Templates", " Email Templates Please think critically when using these canned replies rather than just blindly sending them. Typically, content should be adjusted/ customized for each response to be as relevant, complete, and precise as possible. Remeber to consult the submission guidelines for details of what is expected. Initial email template Hello _____, Thank you for your recent submission to the NSF Arctic Data Center! We will review your data package and get back to you with any suggestions. From my preliminary examination of your submission I have noticed a few items that I would like to bring to your attention. We are here to help you publish your submission, but your continued assistance is needed to do so. See comments below: [COMMENTS HERE] Best, [YOUR NAME] Title Provides the what, where, and when of the data We would like to add some more context to your data package title. A descriptive title that provides context about the what, where, and when of a data package is often far more useful in search results. Something like: ‘OUR SUGGESTION HERE, WHERE, WHEN’ might be an option. Does not use acronyms We noticed that your title contains several acronyms or abbreviations. In order to increase discoverability of your title, please spell out all acronyms and abbreviations. Abstract Describes DATA in package (ideally &gt; 100 words) Your abstract appears to be missing some information. We suggest that the abstract be sufficiently descriptive for a general scientific audience. It should provide an overview of the scientific context/ project/ hypotheses, how this data package fits into the larger project, a synopsis of the experimental or sampling design, and a summary of the data contents. If you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [NSF AWARD URL]. Data At least one data file We noticed that no data files were submitted. With the exception of sensitive social science data, NSF requires the submission of all data products prior to publication. Do you intend to submit data files? Open formats We noticed that the submitted data files are in xlsx format. Please convert your files to a plain text/csv (or other open format) in order to facilitate an accurate transfer of information to users and to ensure preservation of the data in perpetuity. We noticed you submitted your data as a .fig file. While the Arctic Data Center supports the upload of any data file format, sharing data can be greatly enhanced if you use ubiquitous, easy-to-read formats. We recommend conversion of your data to txt or csv (or other open) formats for better archival. File contents and relationships among files are clear Could you provide a short description of the files submitted? Information about how each file was generated (what software, source files, etc.) allows other scientists to reproduce your work. Missing value codes What do the NA’s in your measurements represent? (instrument failure, site not found, etc.) We noticed that the data files contain blank cells. What do these represent? Funding If there are multiple data packages with the same funding number, check about nesting Thank you for your submission to the Arctic Data Center! Based on the NSF awards, your most recent submission and the [PACKAGE NAME] package appear to be related. Would you like your submission(s) grouped with the other data packages funded by the same award? If so, we are happy to do all this grouping for you. Thank you for your submission to the Arctic Data Center! Based on the NSF awards, your most recent submission and the [PACKAGE NAME] parent package seem like disparate projects, is that correct? Nesting data packages under a common “parent” is beneficial so that all packages funded by the same award can be found in one place. This way additional packages can be added to the group without altering existing ones. Once we process all the “child” data packages you upload, we can group them under the same “parent”. The parent does not need to contain any data files itself, but will be populated with metadata only. Methods Submissions should: - provide instrument names - specify how sampling locations were chosen - provide citations for sampling methods that are not explained in detail Your methods section appears to be missing some information. Enough detail should be included so that a reasonable scientist can interpret the study and data for reuse without consulting you nor any other resources. This should hold true today, or even decades from now. Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files. Please edit the data package to provide a more robust methods section. NSF requires that comprehensive methods information be included directly in the metadata record. Pointers or URLs to other sites are unstable and insufficient. Attributes I noticed that your data package has incomplete attribute/variable information. Please add attribute/variable information to each of your data files. You can do so by navigating to your dataset [INSERT URL HERE], clicking the green “Edit” button, clicking the “Describe” button located to the right of each of your files at the top of the submission page, and in the window that pops up click over to the “Attributes” tab. Please see the guidance below for what attribute/variable information should contain. For data packages with tabular data (e.g. CSV files), submitted metadata should contain the following components for every attribute (attributes may also be known as variables and in tabular data are arranged in either columns or rows). Note, storage of data in a long versus wide format will allow for much more succinct metadata. Also, please note that tabular data stored within structured formats such as zip, NetCDF, or ESRI spatial files should also comply with these guidelines. A name (often the column or row header in the file). As with file names, only letters, numbers, hyphens (“-“), and underscores (“_”) should be used in attribute names. Always avoid using spaces and specialized ASCII characters when naming attributes. A complete definition. Any missing value codes along with explanations for those codes. For all numeric data, unit information is needed. For all date-time data, a date-time format is needed (e.g. “DD-MM-YYYY”). For text data, full descriptions for all patterns/codes are needed if the text is constrained to a list of patterns or codes (e.g. a phone number would be constrained to a certain pattern and abbreviations for site locations may be constrained to a list of codes). Could you describe ____? Please define “XYZ”, including the unit of measure. What are the units of measurement for the columns labeled “ABC” and “XYZ”? Final email templates Asking for approval Hi [submitter], I have updated your data package and you can view it here after logging in: [URL] Please review and approve it for publishing or let us know if you would like anything else changed. For your convenience, if we do not hear from you within a week we will proceed with publishing. DOI and data package finalization comments Replying to questions about DOIs &gt; We attribute DOIs to data packages as one might give a DOI to a citable publication. Thus, a DOI is permanently associated with a unique and immutable version of a data package. If the data package changes, a new DOI will be created and the old DOI will be preserved with the original version. DOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to that latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists. Clarification of updating with a DOI and version control &gt; We definitely support updating a data package that has already been assigned a DOI, but when we do so we mark it as a new revision that replaces the original and give it its own DOI. We do that so that any citations of the original version of the data package remain valid (i.e.: after the update, people still know exactly which data were used in the work citing it). Sending finalized URL before resolving ticket [NOTE: the URL format is very specific here, please try to follow it exactly (but substitute in the actual DOI of interest)] &gt; Here is the link to your finalized data package: https://doi.org/10.18739/A20X0X Please let us know if you need any further assistance. New Submission: Abstract, methods, excel to csv, and attributes &gt; Thank you for your submission to the Arctic Data Center. From my preliminary examination of your dataset a few fields need to be updated before we can assign a DOI. Your abstract appears to be missing some information. We suggest that the abstract be sufficiently descriptive for a general scientific audience. It should provide an overview of the scientific context/ project/ hypotheses, how this data package fits into the larger project, a synopsis of the experimental or sampling design, and a summary of the data contents. If you prefer and it is appropriate, we could add language from the abstract in the NSF Award found here: [INSERT URL] Your methods section appears to be missing some information. Enough detail should be included so that a reasonable scientist can interpret the study and data for reuse without consulting you nor any other resources. This should hold true today, or even decades from now. Users should be able to understand how the data were collected, how to interpret the values, and potentially how to use the data in the case of specialized files. Please edit the data package to provide a more robust methods section. We require that comprehensive methods information be included directly in the metadata record. Pointers or URLs to other sites are unstable and insufficient. Your submitted data files are in excel format. Please convert your files to a plain text/csv (or other open source format) in order to facilitate an accurate transfer of information to users and to ensure preservation of the data in perpetuity. We can keep your excel files in the dataset, however we recommend at least one version of each file be stored in an open format. Please add attribute (column) metadata for your data files. If any files share the same attributes you only need to enter them once and we will replicate them to the files. You can add this by navigating to your dataset [INSERT URL], clicking the green “Edit” button, clicking the “Describe” button located to the right of each of your files at the top of the submission page, and in the window that pops up click over to the “Attributes” tab. Additional email templates Deadlines If the PI is checking about dates/timing: We process submissions in the order in which they are received, and yours still has a few ahead of it in our queue. Are you facing any deadlines? If so, we may be able to expedite publication of your submission. Pre-assigned DOI If the PI needs a DOI right away: We can provide you with a pre-assigned DOI that you can reference in your paper. However, please note that it will not become active until after we have finished processing your submission and the package is published. Best practices We noticed that the data files submitted are in _____ format. We recommend conversion of these files into a plain text/csv (OR ANOTHER APPROPRIATE OPEN-SOURCE) format in order to facilitate an accurate transfer of information to future researchers and ensure preservation of the data in perpetuity. Below are some linked articles about data science best practices that the NSF Arctic Data Center adheres to: DataONE - https://www.dataone.org/best-practices “Some Simple Guidelines for Effective Data Management” - http://onlinelibrary.wiley.com/doi/10.1890/0012-9623-90.2.205/full “Good Enough Practices in Scientific Computing” - http://arxiv.org/pdf/1609.00037v1.pdf Adding metadata via R KNB does not support direct uploading of EML metadata files through the website (we have a webform that creates metadata), but you can upload your data and metadata through R! Here are some training materials we have that use both the dataone and datapack packages. It explains how to set your authentication token, build a package from metadata and data files, and publish the package to one of our test sites. I definitely recommend practicing on a test site prior to publishing to the production site your first time through. You can point to the KNB test node (dev.nceas.ucsb.edu) using this command: d1c &lt;- D1Client(&quot;STAGING2&quot;, &quot;urn:node:mnTestKNB&quot;) If you prefer, there are Java, Python, MATLAB, and Bash/cURL clients as well. Nesting Nesting data packages under a common “parent” is beneficial so that all data packages funded by the same award can be found in one place. This way additional data packages can be added to the group without altering existing ones. Once we process all the “child” data packages you upload, we can group them under the same “parent”. The parent will not contain any data files itself, but will be populated with metadata only. Please view an example of a parent data package here. Would you like your submission(s) grouped with the other data packages funded by the same award? If so, we are happy to do all this grouping for you. Finding multiple data packages If linking to multiple data packages, you can send a link to the profile associated with the submitter’s ORCID iD and it will display all their data packages. e.g.: https://arcticdata.io/catalog/#profile/http://orcid.org/0000-0002-2604-4533 NSF ARC data submission policy The NSF Section for Arctic Sciences (ARC) program managers check compliance with their data policies when checking annual reports. Generally, NSF ARC requires that fully quality controlled data be uploaded within 6 months of collection for Arctic Observing Network (AON) projects, or within 2 years of collection (or by end of the grant) for other ARC funded projects. Additionally, complete metadata must be submitted within two years of collection or before the end of the award, whichever comes first. (NSF policies include special exceptions for Arctic Social Sciences Program (ASSP) awards and other awards that contain sensitive data, including human subjects data and data that are governed by an Institutional Review Board policy. These special conditions exist for sharing social science data that are ethically or legally sensitive or at risk of decontextualization.) Please find an overview of the NSF ARC policies here and the full policy information here. Investigators should upload their data to the Arctic Data Center, or, where appropriate, to another community endorsed data archive that ensures the longevity, interpretation, public accessibility, and preservation of the data (e.g., GenBank, NCEI). Local and university web pages generally are not sufficient as an archive. Data preservation should be part of the institutional mission and data must remain accessible even if funding for the archive wanes (i.e., succession plans are in place). We would be happy to discuss the suitability of various archival locations with you further. In order to provide a central location for discovery of ARC-funded data, a metadata record must always be uploaded to the Arctic Data Center even when another community archive is used. Linking ORCiD and LDAP accounts First create an account at orcid.org/register if you have not already. After that account registration is complete, login to the KNB with your ORCID iD here: https://knb.ecoinformatics.org/#share. Next, hover over the icon on the top right and choose “My Profile”. Then, click the “Settings” tab and scroll down to “Add Another Account”. Enter your name or username from your Morpho account and select yourself (your name should populate as an option). Click the “+”. You will then need to log out of knb.ecoinformatics.org and then log back in with your old LDAP account (click “have an existing account”, and enter your Morpho credentials with the organization set to “unaffiliated”) to finalize the linkage between the two accounts. Navigate to “My Profile” and “Settings” to confirm the linkage. After completing this, all of your previously submitted data pacakges should show up on your KNB “My Profile” page, whether you are logged in using your ORCiD or Morpho account, and you will be able to submit data either using Morpho or our web interface. Or, try reversing my instructions - log in first using your Morpho account (by clicking the “existing account” button and selecting organization “unaffiliated”), look for your ORCiD account, then log out and back in with ORCiD to confirm the linkage. "],
["pi-faq-email-templates.html", "PI FAQ Email Templates", " PI FAQ Email Templates Q: Can I replace data that have already been uploaded and keep the DOI? &gt; A: Once you have published your data with the Arctic Data Center, it can still be updated by providing an additional version which can replace the original, while still preserving the original and making it available to anyone who might have cited it. To update your data, return to the data submission tool used to submit it, and provide an update. Any update to a data package qualifies as a new version and therefore requires a new DOI. This is because each DOI represents a unique, immutable version, just like for a journal article. DOIs and URLs for previous versions of data packages remain active on the Arctic Data Center (will continue to resolve to the data package landing page for the specific version they are associated with), but a clear message will appear at the top of the page stating that “A newer version of this dataset exists” with a hyperlink to the latest version. With this approach, any past uses of a DOI (such as in a publication) will remain functional and will reference the specific version of the data package that was cited, while pointing users to the newest version if one exists. Q: Why don’t I see my data package on the ADC? &gt; Possible Answer #1: The data package is still private because we are processing it or awaiting your approval to publish it. Please login with your ORCID iD to view private data packages. Possible Answer #2: The data package is still private and you do not have access because you were not the submitter. If you need access please have the submitter send us a message from his/her email address confirming this, along with your ORCID iD. Once we receive that confirmation we will be happy to grant you permission to view and edit the data package. Possible Answer #3: The data package is still private and we accidentally failed to grant you access. We apologize for the mistake. We have since updated the access policy. Please let us know if you are still having trouble viewing the data package here: [URL]. Remember to login with your ORCID iD. Issue: MANY files to upload (100s or 1000s) or large cumulative size. &gt; A: Can you upload the files to a drive we can access, such as Google Drive or Dropbox? Alternatively, if you have a publicly accessible FTP you can point us to, we could grab the files from there. If needed, we have a secure FTP you can access. Details are available here: https://help.nceas.ucsb.edu/remote_file_access. Please access our server at datateam.nceas.ucsb.edu with the username “visitor”. Let us know if you would like to use our SFTP and we will send you the password and the path to which directory to upload to. If you have files to transfer to us that total several terabytes it may be best to arrange a shipment of an external hard drive. Q: May another person (e.g. my student) submit data using my ORCID iD so that it is linked to me? &gt; A: We recommend instead that the student set up their own ORCiD account at https://ORCiD.org/register and submit data packages from that account. Submissions are processed by our team and, at that point, we can grant you full rights to the metadata and data files even though another person submitted them. Issue: Web form not cooperating. &gt; A: I apologize that you are experiencing difficulties while attempting to submit your data package. We are happy to attempt to troubleshoot this for you. Which operating system (including the version) and browser (with version #) are you using? At which exact step did the issue arise? What error message did you receive? Please provide us with any relevant screenshots. Do you have any rason to believe that you may be using a slow internet connection? Q: May I submit a non-NSF-funded data package? &gt; A: Yes, you can submit non-NSF-funded Arctic data if you are willing to submit under the licensing terms of the Arctic Data Center (CC-0 or CC-BY), the data are moderately sized (with exact limits open to discussion), and a lot of support time to curate the submission is not required (i.e., you submit a complete metadata record and well formatted, open format data files). For larger data packages, we would likely need to charge a one-time archival fee which amortizes the long-term costs of preservation in a single payment. Also, please note that NSF-funded projects take priority when it comes to processing. Information on best practices for data and metadata organization is available here: https://arcticdata.io/submit/#organizing-your-data. Q: Can I add another data file to an existing submission without having to fill out another metadata form? &gt; A: Yes. Navigate to the data package after being sure to login. Then click the green “Edit” button. The form will populate with the already existing metadata so there is no need to fill it out again. Click “Add Files” and browse to the file you wish to add. Be aware that the DOI will change after you add this file (or make any changes to a data package) as, just like for a journal article, a DOI represents a unique and immutable version. The original URL and DOI will remain functional and valid, but clearly display a message at the top of that page stating that “A newer version of this dataset exists” with a link to the latest version. Only the newest version wil be discoverable via a search. Q: Can we submit data as an Excel file? &gt; A: While the Arctic Data Center supports the upload of any data file format, sharing data can be greatly enhanced if you use ubiquitous, easy-to-read formats. For instance, while Microsoft Excel files are commonplace, it is better to export these spreadsheets to Comma Separated Values (CSV) text files, which can be read on any computer without needing to have Microsoft products installed. &gt; So, yes, you are free to submit an Excel workbook, however we strongly recommend converting each sheet to a CSV. The goal is not only for users to be able to read data files, but to be able to analyze them with software, such as R Studio. Typically, we would extract any plots and include them as separate image files. [ONLY SAY THIS NEXT PART IF THE REQUESTOR CONTINUES TO INSIST and then USE PROV TO POINT FROM THE XLS TO THE CSVs] &gt; I understand that having the plots in the same file as the data they are built from simplifies organization. If you definitely prefer to have the Excel workbook included, we ask that you allow us to document the data in both formats and include a note in the metadata clarifying that the data are indeed duplicated (but in different formats). "],
["initial-review-checklist.html", "Initial review checklist", " Initial review checklist Before responding to a new submission use this checklist to review the submission. When your are ready to respond use the initial email template and insert comments and modify as needed. Title Is descriptive of the work (provides enough information to understand the contents at a general scientific level) Provides a location of the work Provides a time frame of the work Abstract Describes the DATA Data At least one data file No xls/xlsx files (or other proprietary files) File contents and relationships among files are clear All attributes are clearly defined. Quality control any dimensionless units. Column names do not use spaces or special characters NA’s are explained Funding At least one funding number If there are multiple submissions with the same funding number/creators, check about nesting Methods Enough detail is provided in metadata Contacts At least one contact with email and ORCiD Temporal/geographic coverage Includes coverage that makes sense "],
["navigate-rt.html", "Navigate RT", " Navigate RT The RT ticketing system is how we communicate with folks interacting with the Arctic Data Center. We use it for managing submissions, accessing issues, etc. It consists of three separate interfaces: Front Page All Tickets Ticket Page Front page This is what you see first Home - brings you to this homepage Tickets - to search for tickets (also see number 5) Tools - not needed New Ticket - create a new ticket Search - Type in the ticket number to quickly navigate to a ticket Queue - Lists all of the tickets currently in a particular queue (such as ‘arcticdata’) and their statuses New = unopened tickets that require attention Open = tickets currently open and under investigation and/or being processed by a support team member Stalled = tickets awaiting responses from the PI/ submitter Tickets I Own - These are the current open tickets that are claimed by me Unowned Tickets - Newest tickets awaiting claim Ticket Status - Status and how long ago it was created Take - claim the ticket as yours All tickets This is the queue interface from number 6 of the Front page 1. Ticket number and title 2. Ticket status 3. Owner - who has claimed the ticket Example ticket Title - Include the PI’s name for reference Display - homepage of the ticket History - Comment/Email history, see bottom of Display page Basics - edit the title, status, and ownership here People - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters Links - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to another ticket number Actions Reply - message the submitter/ PI/ all watchers Comment - attach internal message (no submitters, only Data Teamers) Open It - Open the ticket Stall - submitter has not responded in greater than 1 month Resolve - ticket completed History - message history and option to reply (to submitter and beyond) or comment (internal message) "],
["miscellaneous-file-types.html", "Miscellaneous file types ", " Miscellaneous file types "],
["datalogger-files.html", "Datalogger files", " Datalogger files .hobo files are raw data files offloaded from a sensor, and can include real-time graphs of measured variables, such as temperature and soil moisture conditions. .hobo files can be opened using HOBOware, which can be downloaded from here. "],
["netcdfs.html", "NetCDFs", " NetCDFs This section is for dealing with NetCDF (.nc) files. These files require data tables but since they can not be simply opened on the computer using a text editor or Excel, you can use Panoply to explore them or the R commands. library(ncdf4) # gets attribute info atts &lt;- arcticdatautils::get_ncdf4_attributes(&#39;filepath&#39;) # preview of View(atts) atts[1:10,] # returns the actual values for a specified attribute t &lt;- ncdf4::nc_open(filepath) test &lt;- ncdf4::ncvar_get(t, &#39;attributeName&#39;) # preview of View(test) test[1:10] The formatId in the sysmeta will most likely be netCDF-4, but could be netCDF-3. "],
["spatial-data.html", "Spatial data", " Spatial data Shapefiles and their associated files can be kept in a single grouping using *.zip (or another similar format). To extract meatadata, simply run: library(rgdal) folder_path &lt;- &quot;/folder/path/dir1/dir2/&quot; folder_name &lt;- &quot;folder_name&quot; #name of folder containing shapefiles and associated data spatial_obj &lt;- readOGR(dsn = paste0(folder_path, folder_name), layer = folder_name) spatial_df &lt;- spatial_obj@data #save data as a dataframe spatial_obj@proj4string #get coordinate unit information "],
["wrangle-data.html", "Wrangle data ", " Wrangle data "],
["clean-column-names.html", "Clean column names", " Clean column names You might have read that column names should not include spaces or special characters. Inevitably, you’ll encounter data that are not so tidy. For example: ## First Name Last Name date of birth ## 1 Homer Simpson 1/1/1960 ## 2 Marge Simpson 10/20/1965 ## 3 Ned Flanders 3/22/1961 To tidy it up, you can use the clean_names() function from the janitor package: janitor::clean_names(df) ## first_name last_name date_of_birth ## 1 Homer Simpson 1/1/1960 ## 2 Marge Simpson 10/20/1965 ## 3 Ned Flanders 3/22/1961 "],
["fix-excel-dates.html", "Fix Excel dates", " Fix Excel dates Do you see something that looks like 43134, 43135, 43136 even though the column header is Dates? You may have encountered an Excel date/time problem. To fix it, the janitor package has a handy function: janitor::excel_numeric_to_date(c(43134, 43135, 43136)) ## [1] &quot;2018-02-03&quot; &quot;2018-02-04&quot; &quot;2018-02-05&quot; Make sure you check that the dates make sense! "],
["solr-queries.html", "Solr queries", " Solr queries Solr is what’s known as an index, which helps us keep track of the data, metadata, and resource map objects stored in our metadata catalog, Metacat. Solr allows you to quickly search through any coordinating or member node. Some more common use-cases include: Recovering lost PIDs - if you publish new data objects (publish_object) but forget to save the PIDs, you can recover them with a query. For example, you can search for all data objects that are not associated with a resource map and are published using your ORCiD (q=-resourceMap:*+AND+submitter:*XXXX-YYYY-ZZZZ-WWWW) Working with groups of packages - if you want to find all data, metadata, and resource maps associated with a PI, you can use their last name (q=origin:*SURNAME*) or perhaps their ORCiD (q=rightsHolder:*XXXX-YYYY-ZZZZ-WWWW) or both (q=origin:*SURNAME*+OR+rightsHolder:*XXXX-YYYY-ZZZZ-WWWW) to retrieve information about packages associated with them Once you understand the logic of queries, it becomes a flexible and useful tool that you can integrate into your R workflow. You can use queries to answer a variety of interesting questions, for example: What are the most recently updated data sets? What metadata and data objects are in a given data package? What is the total size (in terms of disk space) of all objects stored in Metacat? Querying Solr is possible by adding a query onto the end of a base URL or through the dataone::query() function in R. For now, we’ll just cover the basics of Solr queries in R. "],
["construct-a-query.html", "Construct a query", " Construct a query Each Solr query is comprised of a number of parameters. These are like arguments to a function in R, but they are entered as parts of a URL. The most common parameters are: q: The query. This is like subset or dplyr::filter in R. fl: What fields are returned for the documents that match your query (q). If not set, all fields are returned. rows: The maximum number of documents to return. Solr will truncate your result if the result size is greater than rows. sort: Sorts the result by the values in the given Solr field (e.g., sort by date uploaded). The query (q) parameter uses a syntax that looks like field:value, where field is one of the Solr fields and value is an expression. The expression can match a specific value exactly, e.g., q=identifier:arctic-data.7747.1 or q=identifier:&quot;doi:10.5065/D60P0X4S&quot;, which finds the Solr document for a specific Object by PID (identifier). In the second example, the DOI PID is surrounded in double quotes. This is because Solr has reserved characters, of which : is one, so we have to help Solr by surrounding values with reserved characters in them in quotes or by escaping them. To view the list of query-able parameters on the Arctic Data Center and their descriptions, you can visit https://arcticdata.io/metacat/d1/mn/v2/query/solr. The list of parameters is also provided below: ## [1] &quot;solr&quot; &quot;abstract&quot; ## [3] &quot;attribute&quot; &quot;attributeDescription&quot; ## [5] &quot;attributeLabel&quot; &quot;attributeName&quot; ## [7] &quot;attributeUnit&quot; &quot;author&quot; ## [9] &quot;authorGivenName&quot; &quot;authorGivenNameSort&quot; ## [11] &quot;authoritativeMN&quot; &quot;authorLastName&quot; ## [13] &quot;authorSurName&quot; &quot;authorSurNameSort&quot; ## [15] &quot;beginDate&quot; &quot;blockedReplicationMN&quot; ## [17] &quot;changePermission&quot; &quot;checksum&quot; ## [19] &quot;checksumAlgorithm&quot; &quot;class&quot; ## [21] &quot;contactOrganization&quot; &quot;contactOrganizationText&quot; ## [23] &quot;datasource&quot; &quot;dataUrl&quot; ## [25] &quot;dateModified&quot; &quot;datePublished&quot; ## [27] &quot;dateUploaded&quot; &quot;decade&quot; ## [29] &quot;documents&quot; &quot;eastBoundCoord&quot; ## [31] &quot;edition&quot; &quot;endDate&quot; ## [33] &quot;family&quot; &quot;fileID&quot; ## [35] &quot;fileName&quot; &quot;formatId&quot; ## [37] &quot;formatType&quot; &quot;gcmdKeyword&quot; ## [39] &quot;genus&quot; &quot;geoform&quot; ## [41] &quot;geohash_1&quot; &quot;geohash_2&quot; ## [43] &quot;geohash_3&quot; &quot;geohash_4&quot; ## [45] &quot;geohash_5&quot; &quot;geohash_6&quot; ## [47] &quot;geohash_7&quot; &quot;geohash_8&quot; ## [49] &quot;geohash_9&quot; &quot;id&quot; ## [51] &quot;identifier&quot; &quot;investigator&quot; ## [53] &quot;investigatorText&quot; &quot;isDocumentedBy&quot; ## [55] &quot;isPublic&quot; &quot;isService&quot; ## [57] &quot;isSpatial&quot; &quot;keyConcept&quot; ## [59] &quot;keywords&quot; &quot;keywordsText&quot; ## [61] &quot;kingdom&quot; &quot;LTERSite&quot; ## [63] &quot;mediaType&quot; &quot;mediaTypeProperty&quot; ## [65] &quot;namedLocation&quot; &quot;noBoundingBox&quot; ## [67] &quot;northBoundCoord&quot; &quot;numberReplicas&quot; ## [69] &quot;obsoletedBy&quot; &quot;obsoletes&quot; ## [71] &quot;ogcUrl&quot; &quot;order&quot; ## [73] &quot;origin&quot; &quot;originator&quot; ## [75] &quot;originatorText&quot; &quot;originText&quot; ## [77] &quot;parameter&quot; &quot;parameterText&quot; ## [79] &quot;phylum&quot; &quot;placeKey&quot; ## [81] &quot;preferredReplicationMN&quot; &quot;presentationCat&quot; ## [83] &quot;project&quot; &quot;projectText&quot; ## [85] &quot;prov_generated&quot; &quot;prov_generatedByExecution&quot; ## [87] &quot;prov_generatedByProgram&quot; &quot;prov_generatedByUser&quot; ## [89] &quot;prov_hasDerivations&quot; &quot;prov_hasSources&quot; ## [91] &quot;prov_instanceOfClass&quot; &quot;prov_used&quot; ## [93] &quot;prov_usedByExecution&quot; &quot;prov_usedByProgram&quot; ## [95] &quot;prov_usedByUser&quot; &quot;prov_wasDerivedFrom&quot; ## [97] &quot;prov_wasExecutedByExecution&quot; &quot;prov_wasExecutedByUser&quot; ## [99] &quot;prov_wasGeneratedBy&quot; &quot;prov_wasInformedBy&quot; ## [101] &quot;pubDate&quot; &quot;purpose&quot; ## [103] &quot;readPermission&quot; &quot;relatedOrganizations&quot; ## [105] &quot;replicaMN&quot; &quot;replicationAllowed&quot; ## [107] &quot;replicaVerifiedDate&quot; &quot;resourceMap&quot; ## [109] &quot;rightsHolder&quot; &quot;scientificName&quot; ## [111] &quot;sem_annotated_by&quot; &quot;sem_annotates&quot; ## [113] &quot;sem_annotation&quot; &quot;sem_comment&quot; ## [115] &quot;sensor&quot; &quot;sensorText&quot; ## [117] &quot;seriesId&quot; &quot;serviceCoupling&quot; ## [119] &quot;serviceDescription&quot; &quot;serviceEndpoint&quot; ## [121] &quot;serviceInput&quot; &quot;serviceOutput&quot; ## [123] &quot;serviceTitle&quot; &quot;serviceType&quot; ## [125] &quot;site&quot; &quot;siteText&quot; ## [127] &quot;size&quot; &quot;sku&quot; ## [129] &quot;source&quot; &quot;sourceText&quot; ## [131] &quot;southBoundCoord&quot; &quot;species&quot; ## [133] &quot;submitter&quot; &quot;term&quot; ## [135] &quot;termText&quot; &quot;text&quot; ## [137] &quot;title&quot; &quot;titlestr&quot; ## [139] &quot;topic&quot; &quot;topicText&quot; ## [141] &quot;updateDate&quot; &quot;webUrl&quot; ## [143] &quot;westBoundCoord&quot; &quot;writePermission&quot; "],
["query-solr-via-a-browser.html", "Query Solr via a browser", " Query Solr via a browser Solr is queried via what’s called an HTTP API (Application Program Interface). Practically, what this means it that you can execute a query in your browser by tacking a query onto a base URL. This is similar to the way Google handles your searches. If I search “soil science” in Google, for example, the URL becomes: https://www.google.com/search?q=soil+science&amp;oq=soil+science&amp;aqs=chrome.0.69i59.1350j0j1&amp;sourceid=chrome&amp;ie=UTF-8 If I break it down into pieces, I get: the base URL - https://www.google.com/search ?, after which the query parameters are listed the query - q=soil+science other parameters, which are separated by &amp; - oq=soil+science&amp;aqs=chrome.0.69i59.1350j0j1&amp;sourceid=chrome&amp;ie=UTF-8 Most of the time, you’ll query either the Arctic Data Center member node or the PROD coordinating node, which have the following base URLs: Arctic Data Center member node: https://arcticdata.io/metacat/d1/mn/v2/query/solr PROD coordinating node: https://cn.dataone.org/cn/v2/query/solr You can then append your query parameters to your base URL: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q={QUERY}&amp;fl={FIELDS}&amp;rows={ROWS} Visit the base URL to see a list of fields Solr is storing for the objects it indexes. There is a large set of queryable fields, though not all types of objects will have values set for all of the possible fields because some fields do not make sense for some objects (e.g., title for a CSV). "],
["query-solr-through-r.html", "Query Solr through R", " Query Solr through R Though it’s possible to query Solr directly through its HTTP API, we typically run our queries through R, for two main reasons: The result is returned in a more useful way to R without extra work on your part We can more easily pass our authentication token with the query Why does #2 matter? Well by default, all of those URLs above only return publicly-readable Solr documents. If a private document matches any of those queries, Solr won’t tell you that. It will act like the non-public-readable documents don’t exist. So we must pass an authentication token to access non-public-readable content. This bit is crucial for working with the ADC, so you’ll very often want to use R instead of visiting those URLs in a web browser. cn &lt;- CNode(&quot;PROD&quot;) adc &lt;- getMNode(cn, &quot;urn:node:ARCTIC&quot;) # Set your token if you need/want! #string your parameters together like this: dataone::query(mn, &quot;q=title:*soil*&amp;fl=title&amp;rows=10&quot;) #or alternatively, list them out: query(adc, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;)) ## [[1]] ## [[1]]$title ## [1] &quot;Chemistry and Water Flow of Thermokarst Impacted Soils, Lakes, and Streams on Alaska&#39;s North Slope&quot; ## ## ## [[2]] ## [[2]]$title ## [1] &quot;Soil organic carbon and microbial lipid radiocarbon content along Alaskan north-south transect&quot; ## ## ## [[3]] ## [[3]]$title ## [1] &quot;Soil Moisture NIMS grid Atqasuk, Alaska 2014&quot; ## ## ## [[4]] ## [[4]]$title ## [1] &quot;Soil Moisture NIMS grid Atqasuk, Alaska 2015&quot; ## ## ## [[5]] ## [[5]]$title ## [1] &quot;Soil Temperature NIMS grid Atqasuk, Alaska 2014&quot; ## ## ## [[6]] ## [[6]]$title ## [1] &quot;Soil Temperature NIMS grid Barrow, Alaska 2014&quot; ## ## ## [[7]] ## [[7]]$title ## [1] &quot;Soil Temperature NIMS grid Barrow, Alaska 2015&quot; ## ## ## [[8]] ## [[8]]$title ## [1] &quot;Discrete soil temperature and volumetric soil water content near Kangerlussuaq, Greenland. 2012-2014&quot; ## ## ## [[9]] ## [[9]]$title ## [1] &quot;Kharp Vegetation and Soil Transects&quot; ## ## ## [[10]] ## [[10]]$title ## [1] &quot;Daily Average Soil, Air and Ground Temperatures - Ivotuk Moist non-Acidic Tundra Site [Romanovsky, V.]&quot; By default, query returns the result as a list, but a data.frame can be a more useful way to work with the result. To get a data.frame instead, just set the as argument to ‘data.frame’ to get a data.frame: query(adc, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;), as = &quot;data.frame&quot;) ## title ## 1 Chemistry and Water Flow of Thermokarst Impacted Soils, Lakes, and Streams on Alaska&#39;s North Slope ## 2 Soil organic carbon and microbial lipid radiocarbon content along Alaskan north-south transect ## 3 Soil Moisture NIMS grid Atqasuk, Alaska 2014 ## 4 Soil Moisture NIMS grid Atqasuk, Alaska 2015 ## 5 Soil Temperature NIMS grid Atqasuk, Alaska 2014 ## 6 Soil Temperature NIMS grid Barrow, Alaska 2014 ## 7 Soil Temperature NIMS grid Barrow, Alaska 2015 ## 8 Discrete soil temperature and volumetric soil water content near Kangerlussuaq, Greenland. 2012-2014 ## 9 Kharp Vegetation and Soil Transects ## 10 Daily Average Soil, Air and Ground Temperatures - Ivotuk Moist non-Acidic Tundra Site [Romanovsky, V.] "],
["use-facets.html", "Use facets", " Use facets We can also summarize what’s in Solr with faceting, which lets us group Solr documents together and count them. This is like table in R. Faceting can do a query within a query, but more commonly it’s used to summarize unique values in a field. For example, we can find the unique formatIds on data objects: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=*:*&amp;fq=formatType:DATA&amp;facet=true&amp;facet.field=formatId&amp;rows=0 To facet, we usually do a few things: Add the parameter facet=true Add the parameter facet.field={FIELD} with the field we want to facet (group) on Set rows=0 because we don’t care about the matched Solr documents Optionally specify fq={expression} which filters out Solr documents before faceting. In the above example, we have to do this to count only data objects. Without it, the facet result would include formatIds for metadata and resource maps, which we don’t want. Currently, the dataone::query() function does not support faceting, so you’ll have to run your queries as a URL. For additional ways to use faceting (such as pivot faceting), check out the Solr documentation. "],
["use-stats.html", "Use stats", " Use stats With stats, we can have Solr calculate statistics on numerical values (such as size). https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=formatType:DATA&amp;stats=true&amp;stats.field=size&amp;rows=0 This query calculates a set of summary statistics for the size field on data objects that Solr has indexed. In this case, Solr’s size field indexes the size field in the system metadata for each object in Metacat. "],
["example-solr-queries.html", "Example Solr queries", " Example Solr queries For additional examples and explanations, check out the Apache Lucene Query Parser Syntax page. Find everything result &lt;- query(adc, list(q=&quot;*:*&quot;, fl=&quot;*&quot;, rows=&quot;20&quot;), as = &quot;data.frame&quot;) Query a wildcard expression #find any id that starts with arctic-data.6 result &lt;- query(adc, list(q=&quot;id:arctic-data.6*&quot;, rows=&quot;5000&quot;), as = &quot;data.frame&quot;) Query multiple fields result &lt;- query(adc, list(q=&quot;title:soil*+AND+origin:Ludwig&quot;, rows=&quot;5000&quot;), as = &quot;data.frame&quot;) result &lt;- query(adc, list(q=&quot;title:soil* OR origin:Ludwig&quot;, rows=&quot;5000&quot;), as = &quot;data.frame&quot;) You can use either spaces or + to separate query parameters. When typing queries in R, it’s often easier to read if you use spaces. However, when using the browser, you may want to use + to keep the query clean. (The browser will replace spaces with %20.) Query pids by a specific submitter result &lt;- query(adc, list(q = &#39;submitter:&quot;http://orcid.org/0000-0003-4703-1974&quot;&#39;, fl = &#39;identifier,submitter,fileName, size&#39;, sort = &#39;dateUploaded+desc&#39;, rows=&#39;1000&#39;), as = &quot;data.frame&quot;) Query pids with special characters # Wrap the pid with special characters with escaped backslashes dataone::query(adc, list(q = paste0(&#39;id:&#39;, &#39;\\&quot;&#39;, &#39;doi:10.18739/A20R9M36V&#39;, &#39;\\&quot;&#39;), fl = &#39;dateUploaded AND identifier&#39;, rows = 5000), as = &quot;data.frame&quot;) Query multiple conditions within one field result &lt;- query(adc, list(q=&quot;title:(soil* AND carbo*)&quot;, rows=&quot;5000&quot;), as = &quot;data.frame&quot;) Query for latest versions only result &lt;- query(adc, list(q = &quot;rightsHolder:*orcid.org/0000-000X-XXXX-XXXX* AND (*:* NOT obsoletedBy:*)&quot;, fl = &quot;identifier,rightsHolder,formatId&quot;, start =&quot;0&quot;, rows = &quot;1500&quot;), as=&quot;data.frame&quot;) Use NOT in a query Just add - before a query parameter! result &lt;- query(adc, list(q=&quot;title:(soil AND -carbon)&quot;, rows=&quot;5000&quot;), as = &quot;data.frame&quot;) Query a coordinating node result &lt;- query(cn, list(q=&quot;title:soil* AND origin:Ludwig&quot;, rows=&quot;5000&quot;), as = &quot;data.frame&quot;) Use facets All resource maps with &gt; 100 data objects that are not on the Arctic Data Center: https://cn.dataone.org/cn/v2/query/solr/?q=resourceMap:*+AND+-datasource:*ARCTIC*&amp;rows=0&amp;facet=true&amp;facet.field=resourceMap&amp;facet.mincount=100 "],
["more-resources.html", "More resources", " More resources Solr’s The Standard Query Parser docs (high level of detail) Another quick reference: https://wiki.apache.org/solr/SolrQuerySyntax http://www.solrtutorial.com/ "],
["adc-web-submissions.html", "ADC Web Submissions ", " ADC Web Submissions "],
["add-physicals-to-submissions.html", "Add Physicals to Submissions", " Add Physicals to Submissions New submissions made through the web editor will not have any physical sections within the otherEntitys. Add them to the EML with the following script: for (i in seq_along(eml@dataset@otherEntity)) { otherEntity &lt;- eml@dataset@otherEntity[[i]] id &lt;- otherEntity@id if (!grepl(&quot;urn-uuid-&quot;, id)) { warning(&quot;otherEntity &quot;, i, &quot; is not a pid&quot;) } else { id &lt;- gsub(&quot;urn-uuid-&quot;, &quot;urn:uuid:&quot;, id) physical &lt;- arcticdatautils::pid_to_eml_physical(mn, id) eml@dataset@otherEntity[[i]]@physical[[1]] &lt;- physical[[1]] } } "],
["assess-attributes.html", "Assess Attributes", " Assess Attributes New submissions made through the web editor should have attributes created by the submitter. If there are no attributes, or if they are incomplete, please use the email template to correspond with the submitter to ask for attributes. Additonally, the web editor will not allow for the creation of custom units. submitters should select other/none in the editor unit dropdown if they cannot find their desired unit. In the EML document, these will result in dimensionless units. Accordingly, new submissions should be checked for dimensionless units. This can be done with the following code. dim_units &lt;- sapply(eml@dataset@otherEntity, function(x) { i &lt;- datamgmt::which_in_eml(x@attributeList@attribute, &quot;standardUnit&quot;, as(&quot;dimensionless&quot;, &quot;standardUnit&quot;)) out &lt;- sapply(i, function(j){ list(entityName = x@entityName, attributeName = x@attributeList@attribute[[j]]@attributeName, attributeLabel = x@attributeList@attribute[[j]]@attributeLabel, attributeDefinition = x@attributeList@attribute[[j]]@attributeDefinition) }) return(out) }) dim_units &lt;- data.frame(t(do.call(cbind, dim_units))) dim_units If dimensionless units are found, first check the attributeDefinition, attributeLabel, and attributeName to see if dimensionless seems to be an appropriate unit for the attribute. If there is any question about wether or not dimensionless seems appropriate, ask another member of the data team. If there is still any question about what the unit should be, reach out to the submitter and ask for clarification. "]
]
